{
  "type": "pull",
  "pull": {
    "url": "https://api.github.com/repos/bitcoin/bips/pulls/609",
    "id": 151792625,
    "node_id": "MDExOlB1bGxSZXF1ZXN0MTUxNzkyNjI1",
    "html_url": "https://github.com/bitcoin/bips/pull/609",
    "diff_url": "https://github.com/bitcoin/bips/pull/609.diff",
    "patch_url": "https://github.com/bitcoin/bips/pull/609.patch",
    "issue_url": "https://api.github.com/repos/bitcoin/bips/issues/609",
    "commits_url": "https://api.github.com/repos/bitcoin/bips/pulls/609/commits",
    "review_comments_url": "https://api.github.com/repos/bitcoin/bips/pulls/609/comments",
    "review_comment_url": "https://api.github.com/repos/bitcoin/bips/pulls/comments%7B/number%7D",
    "comments_url": "https://api.github.com/repos/bitcoin/bips/issues/609/comments",
    "statuses_url": "https://api.github.com/repos/bitcoin/bips/statuses/d52f586a1309be04e0297e44fa06f6241780e466",
    "number": 609,
    "state": "closed",
    "locked": false,
    "maintainer_can_modify": false,
    "title": "new bip: compact client side filtering",
    "user": {
      "login": "Roasbeef",
      "id": 998190,
      "node_id": "MDQ6VXNlcjk5ODE5MA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Roasbeef",
      "html_url": "https://github.com/Roasbeef",
      "followers_url": "https://api.github.com/users/Roasbeef/followers",
      "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
      "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
      "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
      "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
      "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
      "repos_url": "https://api.github.com/users/Roasbeef/repos",
      "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
      "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
      "type": "User",
      "site_admin": false
    },
    "body": "This BIP describes a new light client node type for Bitcoin as well as the\r\nmodifications to current full-nodes required to support this new type of light\r\nclient. The light client mode described in this BIP is meant to supersede BIP\r\n37 as it provides a greater degree of privacy, utility, and also reduces the\r\nresources required for full-nodes to service this new light client mode\r\ncompared to BIP 37. The light client mode described in this BIP can be seen as\r\na \"reversal\" of BIP 37: rather than the light clients sending filters to\r\nfull-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\r\nutilize bloom filters. Instead, we utilize a compact filter (more efficient\r\nthan bloom filters) which leverages Golomb-Rice coding for compression.\r\nAdditionally, blocks are downloaded as a whole (from any source), rather than\r\ndirectly from peers as fragments with merkle-branches proving their\r\nauthenticity.\r\n\r\nWe've tested the implementation of both [a light client implementing this\r\nBIP](https://github.com/lightninglabs/neutrino), as well as the serving full-node on Bitcoin's testnet over the past several months. Additionally, this new operating mode also serves as the basis for our [light weight Lightning node](https://github.com/lightningnetwork/lnd/).",
    "labels": [],
    "created_at": "2017-11-09T23:42:05Z",
    "updated_at": "2018-01-17T22:30:00Z",
    "closed_at": "2018-01-17T22:30:00Z",
    "mergeable_state": "unknown",
    "merge_commit_sha": "fd5ca1d19fa98647832cd176e74173746f7b7004",
    "assignees": [],
    "requested_reviewers": [],
    "requested_teams": [],
    "head": {
      "label": "Roasbeef:master",
      "ref": "master",
      "sha": "d52f586a1309be04e0297e44fa06f6241780e466",
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "repo": {
        "id": 51169931,
        "node_id": "MDEwOlJlcG9zaXRvcnk1MTE2OTkzMQ==",
        "name": "bips",
        "full_name": "Roasbeef/bips",
        "owner": {
          "login": "Roasbeef",
          "id": 998190,
          "node_id": "MDQ6VXNlcjk5ODE5MA==",
          "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/Roasbeef",
          "html_url": "https://github.com/Roasbeef",
          "followers_url": "https://api.github.com/users/Roasbeef/followers",
          "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
          "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
          "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
          "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
          "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
          "repos_url": "https://api.github.com/users/Roasbeef/repos",
          "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
          "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
          "type": "User",
          "site_admin": false
        },
        "private": false,
        "html_url": "https://github.com/Roasbeef/bips",
        "description": "Bitcoin Improvement Proposals",
        "fork": true,
        "url": "https://api.github.com/repos/Roasbeef/bips",
        "archive_url": "https://api.github.com/repos/Roasbeef/bips/%7Barchive_format%7D%7B/ref%7D",
        "assignees_url": "https://api.github.com/repos/Roasbeef/bips/assignees%7B/user%7D",
        "blobs_url": "https://api.github.com/repos/Roasbeef/bips/git/blobs%7B/sha%7D",
        "branches_url": "https://api.github.com/repos/Roasbeef/bips/branches%7B/branch%7D",
        "collaborators_url": "https://api.github.com/repos/Roasbeef/bips/collaborators%7B/collaborator%7D",
        "comments_url": "https://api.github.com/repos/Roasbeef/bips/comments%7B/number%7D",
        "commits_url": "https://api.github.com/repos/Roasbeef/bips/commits%7B/sha%7D",
        "compare_url": "https://api.github.com/repos/Roasbeef/bips/compare/%7Bbase%7D...%7Bhead%7D",
        "contents_url": "https://api.github.com/repos/Roasbeef/bips/contents/%7B+path%7D",
        "contributors_url": "https://api.github.com/repos/Roasbeef/bips/contributors",
        "deployments_url": "https://api.github.com/repos/Roasbeef/bips/deployments",
        "downloads_url": "https://api.github.com/repos/Roasbeef/bips/downloads",
        "events_url": "https://api.github.com/repos/Roasbeef/bips/events",
        "forks_url": "https://api.github.com/repos/Roasbeef/bips/forks",
        "git_commits_url": "https://api.github.com/repos/Roasbeef/bips/git/commits%7B/sha%7D",
        "git_refs_url": "https://api.github.com/repos/Roasbeef/bips/git/refs%7B/sha%7D",
        "git_tags_url": "https://api.github.com/repos/Roasbeef/bips/git/tags%7B/sha%7D",
        "git_url": "git://github.com/Roasbeef/bips.git",
        "issue_comment_url": "https://api.github.com/repos/Roasbeef/bips/issues/comments%7B/number%7D",
        "issue_events_url": "https://api.github.com/repos/Roasbeef/bips/issues/events%7B/number%7D",
        "issues_url": "https://api.github.com/repos/Roasbeef/bips/issues%7B/number%7D",
        "keys_url": "https://api.github.com/repos/Roasbeef/bips/keys%7B/key_id%7D",
        "labels_url": "https://api.github.com/repos/Roasbeef/bips/labels%7B/name%7D",
        "languages_url": "https://api.github.com/repos/Roasbeef/bips/languages",
        "merges_url": "https://api.github.com/repos/Roasbeef/bips/merges",
        "milestones_url": "https://api.github.com/repos/Roasbeef/bips/milestones%7B/number%7D",
        "notifications_url": "https://api.github.com/repos/Roasbeef/bips/notifications%7B?since,all,participating}",
        "pulls_url": "https://api.github.com/repos/Roasbeef/bips/pulls%7B/number%7D",
        "releases_url": "https://api.github.com/repos/Roasbeef/bips/releases%7B/id%7D",
        "ssh_url": "git@github.com:Roasbeef/bips.git",
        "stargazers_url": "https://api.github.com/repos/Roasbeef/bips/stargazers",
        "statuses_url": "https://api.github.com/repos/Roasbeef/bips/statuses/%7Bsha%7D",
        "subscribers_url": "https://api.github.com/repos/Roasbeef/bips/subscribers",
        "subscription_url": "https://api.github.com/repos/Roasbeef/bips/subscription",
        "tags_url": "https://api.github.com/repos/Roasbeef/bips/tags",
        "teams_url": "https://api.github.com/repos/Roasbeef/bips/teams",
        "trees_url": "https://api.github.com/repos/Roasbeef/bips/git/trees%7B/sha%7D",
        "clone_url": "https://github.com/Roasbeef/bips.git",
        "hooks_url": "https://api.github.com/repos/Roasbeef/bips/hooks",
        "svn_url": "https://github.com/Roasbeef/bips",
        "homepage": "bitcoin.org",
        "language": "Go",
        "forks_count": 23,
        "stargazers_count": 57,
        "watchers_count": 57,
        "size": 13234,
        "default_branch": "master",
        "open_issues_count": 3,
        "is_template": false,
        "topics": [],
        "has_issues": false,
        "has_projects": true,
        "has_wiki": false,
        "has_pages": false,
        "has_downloads": true,
        "archived": false,
        "disabled": false,
        "visibility": "public",
        "pushed_at": "2023-05-30T12:31:52Z",
        "created_at": "2016-02-05T19:45:12Z",
        "updated_at": "2023-05-17T06:31:36Z"
      }
    },
    "base": {
      "label": "bitcoin:master",
      "ref": "master",
      "sha": "5c48bcc5ae2a38d4b180abe2c5bc93e8ccdd0b78",
      "user": {
        "login": "bitcoin",
        "id": 528860,
        "node_id": "MDEyOk9yZ2FuaXphdGlvbjUyODg2MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/528860?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/bitcoin",
        "html_url": "https://github.com/bitcoin",
        "followers_url": "https://api.github.com/users/bitcoin/followers",
        "following_url": "https://api.github.com/users/bitcoin/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/bitcoin/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/bitcoin/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/bitcoin/subscriptions",
        "organizations_url": "https://api.github.com/users/bitcoin/orgs",
        "repos_url": "https://api.github.com/users/bitcoin/repos",
        "events_url": "https://api.github.com/users/bitcoin/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/bitcoin/received_events",
        "type": "Organization",
        "site_admin": false
      },
      "repo": {
        "id": 14531737,
        "node_id": "MDEwOlJlcG9zaXRvcnkxNDUzMTczNw==",
        "name": "bips",
        "full_name": "bitcoin/bips",
        "owner": {
          "login": "bitcoin",
          "id": 528860,
          "node_id": "MDEyOk9yZ2FuaXphdGlvbjUyODg2MA==",
          "avatar_url": "https://avatars.githubusercontent.com/u/528860?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/bitcoin",
          "html_url": "https://github.com/bitcoin",
          "followers_url": "https://api.github.com/users/bitcoin/followers",
          "following_url": "https://api.github.com/users/bitcoin/following%7B/other_user%7D",
          "gists_url": "https://api.github.com/users/bitcoin/gists%7B/gist_id%7D",
          "starred_url": "https://api.github.com/users/bitcoin/starred%7B/owner%7D%7B/repo%7D",
          "subscriptions_url": "https://api.github.com/users/bitcoin/subscriptions",
          "organizations_url": "https://api.github.com/users/bitcoin/orgs",
          "repos_url": "https://api.github.com/users/bitcoin/repos",
          "events_url": "https://api.github.com/users/bitcoin/events%7B/privacy%7D",
          "received_events_url": "https://api.github.com/users/bitcoin/received_events",
          "type": "Organization",
          "site_admin": false
        },
        "private": false,
        "html_url": "https://github.com/bitcoin/bips",
        "description": "Bitcoin Improvement Proposals",
        "fork": false,
        "url": "https://api.github.com/repos/bitcoin/bips",
        "archive_url": "https://api.github.com/repos/bitcoin/bips/%7Barchive_format%7D%7B/ref%7D",
        "assignees_url": "https://api.github.com/repos/bitcoin/bips/assignees%7B/user%7D",
        "blobs_url": "https://api.github.com/repos/bitcoin/bips/git/blobs%7B/sha%7D",
        "branches_url": "https://api.github.com/repos/bitcoin/bips/branches%7B/branch%7D",
        "collaborators_url": "https://api.github.com/repos/bitcoin/bips/collaborators%7B/collaborator%7D",
        "comments_url": "https://api.github.com/repos/bitcoin/bips/comments%7B/number%7D",
        "commits_url": "https://api.github.com/repos/bitcoin/bips/commits%7B/sha%7D",
        "compare_url": "https://api.github.com/repos/bitcoin/bips/compare/%7Bbase%7D...%7Bhead%7D",
        "contents_url": "https://api.github.com/repos/bitcoin/bips/contents/%7B+path%7D",
        "contributors_url": "https://api.github.com/repos/bitcoin/bips/contributors",
        "deployments_url": "https://api.github.com/repos/bitcoin/bips/deployments",
        "downloads_url": "https://api.github.com/repos/bitcoin/bips/downloads",
        "events_url": "https://api.github.com/repos/bitcoin/bips/events",
        "forks_url": "https://api.github.com/repos/bitcoin/bips/forks",
        "git_commits_url": "https://api.github.com/repos/bitcoin/bips/git/commits%7B/sha%7D",
        "git_refs_url": "https://api.github.com/repos/bitcoin/bips/git/refs%7B/sha%7D",
        "git_tags_url": "https://api.github.com/repos/bitcoin/bips/git/tags%7B/sha%7D",
        "git_url": "git://github.com/bitcoin/bips.git",
        "issue_comment_url": "https://api.github.com/repos/bitcoin/bips/issues/comments%7B/number%7D",
        "issue_events_url": "https://api.github.com/repos/bitcoin/bips/issues/events%7B/number%7D",
        "issues_url": "https://api.github.com/repos/bitcoin/bips/issues%7B/number%7D",
        "keys_url": "https://api.github.com/repos/bitcoin/bips/keys%7B/key_id%7D",
        "labels_url": "https://api.github.com/repos/bitcoin/bips/labels%7B/name%7D",
        "languages_url": "https://api.github.com/repos/bitcoin/bips/languages",
        "merges_url": "https://api.github.com/repos/bitcoin/bips/merges",
        "milestones_url": "https://api.github.com/repos/bitcoin/bips/milestones%7B/number%7D",
        "notifications_url": "https://api.github.com/repos/bitcoin/bips/notifications%7B?since,all,participating}",
        "pulls_url": "https://api.github.com/repos/bitcoin/bips/pulls%7B/number%7D",
        "releases_url": "https://api.github.com/repos/bitcoin/bips/releases%7B/id%7D",
        "ssh_url": "git@github.com:bitcoin/bips.git",
        "stargazers_url": "https://api.github.com/repos/bitcoin/bips/stargazers",
        "statuses_url": "https://api.github.com/repos/bitcoin/bips/statuses/%7Bsha%7D",
        "subscribers_url": "https://api.github.com/repos/bitcoin/bips/subscribers",
        "subscription_url": "https://api.github.com/repos/bitcoin/bips/subscription",
        "tags_url": "https://api.github.com/repos/bitcoin/bips/tags",
        "teams_url": "https://api.github.com/repos/bitcoin/bips/teams",
        "trees_url": "https://api.github.com/repos/bitcoin/bips/git/trees%7B/sha%7D",
        "clone_url": "https://github.com/bitcoin/bips.git",
        "hooks_url": "https://api.github.com/repos/bitcoin/bips/hooks",
        "svn_url": "https://github.com/bitcoin/bips",
        "homepage": "",
        "language": "Wikitext",
        "forks_count": 5178,
        "stargazers_count": 8116,
        "watchers_count": 8116,
        "size": 13694,
        "default_branch": "master",
        "open_issues_count": 115,
        "is_template": false,
        "topics": [],
        "has_issues": false,
        "has_projects": false,
        "has_wiki": true,
        "has_pages": false,
        "has_downloads": true,
        "archived": false,
        "disabled": false,
        "visibility": "public",
        "pushed_at": "2023-06-08T12:03:37Z",
        "created_at": "2013-11-19T17:18:41Z",
        "updated_at": "2023-06-12T10:53:19Z"
      }
    },
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
      }
    },
    "author_association": "CONTRIBUTOR",
    "draft": false,
    "additions": 1614,
    "deletions": 0,
    "changed_files": 34,
    "commits": 14,
    "review_comments": 80,
    "comments": 4
  },
  "events": [
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6ODNiODNjNzhlMTg5YmU4OTg1NzNlMGJmZTkzNmRkMGM5Yjk5ZWNiOQ==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "tree": {
        "sha": "118748d1d11598debd367df5504f48b2b52b7eb8",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/118748d1d11598debd367df5504f48b2b52b7eb8"
      },
      "verification": {
        "verified": true,
        "reason": "valid",
        "payload": "tree 118748d1d11598debd367df5504f48b2b52b7eb8\nparent 5c48bcc5ae2a38d4b180abe2c5bc93e8ccdd0b78\nauthor Olaoluwa Osuntokun <laolu32@gmail.com> 1496343559 -0700\ncommitter Olaoluwa Osuntokun <laolu32@gmail.com> 1510270096 -0800\n\nnew bip: compact client side filtering\n",
        "signature": "-----BEGIN PGP SIGNATURE-----\n\niQIcBAABCgAGBQJaBOSRAAoJEJZOomPdY3whHNcQAI9EWXyzSOvCFqnZTK98Uiot\nGc3/uDe0To0CCpeDEDpGmBdDf1gwkvQnmKg5Sd7ZzXtsG1cCQfSpFdTLBLO4WVK5\njWOn1mR8XIdeXppOw4FWm6bYy4q1lHE88N3f1XAzJrCKwNMvKksgTm9KQtbu4+et\nLvm0tY5C5f22iIpADjPD+jNQN1IYPPr4KUzJWFppzIw/hcyRTEr4+FTCVD3ZpPDb\nk1UNFGbPuNPZlXR3HKEswZ4qZ6kxabixAaQ/uFAvvDWw5n2/TWO18I6+ca+rHR30\nv3FeMYMaCTMhzjwWTnVmam22+GAPaCqIBeFHqd6+iN7SKOscXzSV0NJ12aT/xpje\nTVpSbLb4EdzULI77o7ZqV/arOI/tWrMpUPh67ozAzEgboYd5mF/cB2Df+tWL3EJu\n5DTO4wtWx1ScQgf31i3Z+qZDy3Y0BHzO77P/6lyCFUCnK239gGFqqX7rI8xL5YYK\ns2ggfkq4XFWfgc9DVtGAzE8rgshZuOHSVKHSL768AssG7GFd7aRZdPRwZRL61wfj\nsdrkJnsT9xza1H8QaFcRN+TnbuUR0AEb4+Vq38yiGwa3UJFjvT0NfB5ChhX8ZLYQ\n2WyiQ7MzF2hT7maz/HgkHlCT+JaC7J4ifxLpm+v2CgufV6CvWLxSsjEh/EucEjsF\niyk4jOciUWkB8edviaCg\n=0O8S\n-----END PGP SIGNATURE-----"
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/5c48bcc5ae2a38d4b180abe2c5bc93e8ccdd0b78",
          "sha": "5c48bcc5ae2a38d4b180abe2c5bc93e8ccdd0b78",
          "html_url": "https://github.com/bitcoin/bips/commit/5c48bcc5ae2a38d4b180abe2c5bc93e8ccdd0b78"
        }
      ],
      "message": "new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-09T23:28:16Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-06-01T18:59:19Z"
      },
      "sha": "83b83c78e189be898573e0bfe936dd0c9b99ecb9"
    },
    {
      "event": "reviewed",
      "id": 75690121,
      "node_id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzU2OTAxMjE=",
      "url": null,
      "actor": null,
      "commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Needs to address backward compatibility, at least briefly (eg, just say there isn't any).\r\n\r\nWe probably shouldn't copy the test vectors here. The external repo is good for that.",
      "user": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bips/pull/609#pullrequestreview-75690121",
      "submitted_at": "2017-11-10T08:43:24Z",
      "state": "CHANGES_REQUESTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bips/pulls/609"
    },
    {
      "event": "reviewed",
      "id": 75928460,
      "node_id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzU5Mjg0NjA=",
      "url": null,
      "actor": null,
      "commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "commit_url": null,
      "created_at": null,
      "author_association": "CONTRIBUTOR",
      "body": "",
      "user": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bips/pull/609#pullrequestreview-75928460",
      "submitted_at": "2017-11-13T18:53:41Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bips/pulls/609"
    },
    {
      "event": "reviewed",
      "id": 76216067,
      "node_id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzYyMTYwNjc=",
      "url": null,
      "actor": null,
      "commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "I found this spec very confusing...I could probably stumble my way through implementing it purely because of the test vectors, but I would certainly miss parts of it the first few times around. Also, there is way too much background information in the \"Specification\" section - it doesn't read as a specification, it reads as a general goal of what you want it to do.",
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bips/pull/609#pullrequestreview-76216067",
      "submitted_at": "2017-11-13T20:59:19Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bips/pulls/609"
    },
    {
      "event": "reviewed",
      "id": 76880119,
      "node_id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzY4ODAxMTk=",
      "url": null,
      "actor": null,
      "commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "commit_url": null,
      "created_at": null,
      "author_association": "CONTRIBUTOR",
      "user": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bips/pull/609#pullrequestreview-76880119",
      "submitted_at": "2017-11-15T18:17:10Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bips/pulls/609"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6YWY1ZTEzNzZlZmE5ZDM2NzAxNDFlOWVkYzFkMGZkZWFkMWU0N2YyNQ==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/af5e1376efa9d3670141e9edc1d0fdead1e47f25",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/af5e1376efa9d3670141e9edc1d0fdead1e47f25",
      "tree": {
        "sha": "a73cfa1adefbd56e6c9b9e90814990144cc424c6",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/a73cfa1adefbd56e6c9b9e90814990144cc424c6"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/83b83c78e189be898573e0bfe936dd0c9b99ecb9",
          "sha": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
          "html_url": "https://github.com/bitcoin/bips/commit/83b83c78e189be898573e0bfe936dd0c9b99ecb9"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T04:41:30Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T04:41:30Z"
      },
      "sha": "af5e1376efa9d3670141e9edc1d0fdead1e47f25"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6NTlmZDkyZTU1Njg3MzkyN2I1ZjI3YWQwMGU5YjBjMWNjMTM2NzVjMA==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/59fd92e556873927b5f27ad00e9b0c1cc13675c0",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/59fd92e556873927b5f27ad00e9b0c1cc13675c0",
      "tree": {
        "sha": "c373c37480f9037e79be7c1c41a4cb54c323919c",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/c373c37480f9037e79be7c1c41a4cb54c323919c"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/af5e1376efa9d3670141e9edc1d0fdead1e47f25",
          "sha": "af5e1376efa9d3670141e9edc1d0fdead1e47f25",
          "html_url": "https://github.com/bitcoin/bips/commit/af5e1376efa9d3670141e9edc1d0fdead1e47f25"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T05:00:06Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T05:00:06Z"
      },
      "sha": "59fd92e556873927b5f27ad00e9b0c1cc13675c0"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6NmEyZjljZGFjNzU0NWZmMmFhNzBmYzVlNWU5ZmFhY2ZjOTJmNWVjMQ==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/6a2f9cdac7545ff2aa70fc5e5e9faacfc92f5ec1",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/6a2f9cdac7545ff2aa70fc5e5e9faacfc92f5ec1",
      "tree": {
        "sha": "74d80e8fd489edeca426b9131e9b4f37086f67ec",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/74d80e8fd489edeca426b9131e9b4f37086f67ec"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/59fd92e556873927b5f27ad00e9b0c1cc13675c0",
          "sha": "59fd92e556873927b5f27ad00e9b0c1cc13675c0",
          "html_url": "https://github.com/bitcoin/bips/commit/59fd92e556873927b5f27ad00e9b0c1cc13675c0"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T05:00:17Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T05:00:17Z"
      },
      "sha": "6a2f9cdac7545ff2aa70fc5e5e9faacfc92f5ec1"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6MzNiNmY0OWYwOGYxNDU0ZTAzNWMyNTYxYTA1OGQ3MDM5OTgyMzlkMg==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/33b6f49f08f1454e035c2561a058d703998239d2",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/33b6f49f08f1454e035c2561a058d703998239d2",
      "tree": {
        "sha": "e7741016e52431180157a8c1bf45d322a3e3c915",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/e7741016e52431180157a8c1bf45d322a3e3c915"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/6a2f9cdac7545ff2aa70fc5e5e9faacfc92f5ec1",
          "sha": "6a2f9cdac7545ff2aa70fc5e5e9faacfc92f5ec1",
          "html_url": "https://github.com/bitcoin/bips/commit/6a2f9cdac7545ff2aa70fc5e5e9faacfc92f5ec1"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:12:50Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:12:50Z"
      },
      "sha": "33b6f49f08f1454e035c2561a058d703998239d2"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6MGE2ODQ3OWEwYjVmZmVjNGI1ZjI2NjlkMTZhMmU1NzkxNzYxYjUzMA==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/0a68479a0b5ffec4b5f2669d16a2e5791761b530",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/0a68479a0b5ffec4b5f2669d16a2e5791761b530",
      "tree": {
        "sha": "5aff2fb8fbd0a1cfddcfb3e39b774f7756d06f29",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/5aff2fb8fbd0a1cfddcfb3e39b774f7756d06f29"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/33b6f49f08f1454e035c2561a058d703998239d2",
          "sha": "33b6f49f08f1454e035c2561a058d703998239d2",
          "html_url": "https://github.com/bitcoin/bips/commit/33b6f49f08f1454e035c2561a058d703998239d2"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:14:22Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:14:22Z"
      },
      "sha": "0a68479a0b5ffec4b5f2669d16a2e5791761b530"
    },
    {
      "event": "commented",
      "id": 348103445,
      "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODEwMzQ0NQ==",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/comments/348103445",
      "actor": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2017-11-30T07:17:44Z",
      "updated_at": "2017-11-30T07:17:44Z",
      "author_association": "CONTRIBUTOR",
      "body": "@luke-jr AFAICT other BIPs do contain test vectors in folders named the same as the BIP. For example BIP-00039 contains the word list used. Added a section for backwards compatability. \r\n\r\n@TheBlueMatt thanks for the thorough review! Admittedly the initial section reads more like a blog post. Following your suggestion I've adopted RFC words where applicable throughout, and created a new section (\"Mathematical Background\") that houses what use to be the opening portion of the specification. ",
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bips/pull/609#issuecomment-348103445",
      "issue_url": "https://api.github.com/repos/bitcoin/bips/issues/609"
    },
    {
      "event": "mentioned",
      "id": 1365054592,
      "node_id": "MDE0Ok1lbnRpb25lZEV2ZW50MTM2NTA1NDU5Mg==",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/events/1365054592",
      "actor": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2017-11-30T07:17:44Z"
    },
    {
      "event": "subscribed",
      "id": 1365054593,
      "node_id": "MDE1OlN1YnNjcmliZWRFdmVudDEzNjUwNTQ1OTM=",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/events/1365054593",
      "actor": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2017-11-30T07:17:44Z"
    },
    {
      "event": "mentioned",
      "id": 1365054595,
      "node_id": "MDE0Ok1lbnRpb25lZEV2ZW50MTM2NTA1NDU5NQ==",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/events/1365054595",
      "actor": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2017-11-30T07:17:44Z"
    },
    {
      "event": "subscribed",
      "id": 1365054596,
      "node_id": "MDE1OlN1YnNjcmliZWRFdmVudDEzNjUwNTQ1OTY=",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/events/1365054596",
      "actor": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2017-11-30T07:17:44Z"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6NWQ3MTExN2IzZGE5NzM5ZDI3YzVlZWI4N2EzODkwM2VhNTRlOTdjYg==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/5d71117b3da9739d27c5eeb87a38903ea54e97cb",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/5d71117b3da9739d27c5eeb87a38903ea54e97cb",
      "tree": {
        "sha": "63a2f3e4328f3f4b4f238802ed5aaacd648d7c8a",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/63a2f3e4328f3f4b4f238802ed5aaacd648d7c8a"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/0a68479a0b5ffec4b5f2669d16a2e5791761b530",
          "sha": "0a68479a0b5ffec4b5f2669d16a2e5791761b530",
          "html_url": "https://github.com/bitcoin/bips/commit/0a68479a0b5ffec4b5f2669d16a2e5791761b530"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:18:34Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:18:34Z"
      },
      "sha": "5d71117b3da9739d27c5eeb87a38903ea54e97cb"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6MzcyOGVhMGRiNTVmOTAzNzYwZWE1NmMwNzc3N2RjOTQ2NTE4M2NmYw==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/3728ea0db55f903760ea56c07777dc9465183cfc",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/3728ea0db55f903760ea56c07777dc9465183cfc",
      "tree": {
        "sha": "23313a723709dfdf063acdd734ed185fbb2a726e",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/23313a723709dfdf063acdd734ed185fbb2a726e"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/5d71117b3da9739d27c5eeb87a38903ea54e97cb",
          "sha": "5d71117b3da9739d27c5eeb87a38903ea54e97cb",
          "html_url": "https://github.com/bitcoin/bips/commit/5d71117b3da9739d27c5eeb87a38903ea54e97cb"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:19:47Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:19:47Z"
      },
      "sha": "3728ea0db55f903760ea56c07777dc9465183cfc"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6ZGU0YzZjOWNhYmQ1ZDFjNjZhY2E5MmQ0YmIwN2UwZTY1MDgxODgwOQ==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/de4c6c9cabd5d1c66aca92d4bb07e0e650818809",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/de4c6c9cabd5d1c66aca92d4bb07e0e650818809",
      "tree": {
        "sha": "06e84512405fe8644f1355103d743671cae6aade",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/06e84512405fe8644f1355103d743671cae6aade"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/3728ea0db55f903760ea56c07777dc9465183cfc",
          "sha": "3728ea0db55f903760ea56c07777dc9465183cfc",
          "html_url": "https://github.com/bitcoin/bips/commit/3728ea0db55f903760ea56c07777dc9465183cfc"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:20:42Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:20:42Z"
      },
      "sha": "de4c6c9cabd5d1c66aca92d4bb07e0e650818809"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6M2MzMWFmNjU3ZmNmZDdlZDhjODliNGVkNmZjOWVmMmQxMWU1ODMyMQ==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/3c31af657fcfd7ed8c89b4ed6fc9ef2d11e58321",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/3c31af657fcfd7ed8c89b4ed6fc9ef2d11e58321",
      "tree": {
        "sha": "fe83b6d19821a700ca694a6885a2c5d1e2e13568",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/fe83b6d19821a700ca694a6885a2c5d1e2e13568"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/de4c6c9cabd5d1c66aca92d4bb07e0e650818809",
          "sha": "de4c6c9cabd5d1c66aca92d4bb07e0e650818809",
          "html_url": "https://github.com/bitcoin/bips/commit/de4c6c9cabd5d1c66aca92d4bb07e0e650818809"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:21:34Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:21:34Z"
      },
      "sha": "3c31af657fcfd7ed8c89b4ed6fc9ef2d11e58321"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6M2VlOTllNjZhOTM4NjFiMTU2YTliYWMyYjkxNjdiM2FjYzVhMmI4Mg==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/3ee99e66a93861b156a9bac2b9167b3acc5a2b82",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/3ee99e66a93861b156a9bac2b9167b3acc5a2b82",
      "tree": {
        "sha": "5e9a88c2a719c9ad657dc3b4e964407ec15ae2a3",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/5e9a88c2a719c9ad657dc3b4e964407ec15ae2a3"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/3c31af657fcfd7ed8c89b4ed6fc9ef2d11e58321",
          "sha": "3c31af657fcfd7ed8c89b4ed6fc9ef2d11e58321",
          "html_url": "https://github.com/bitcoin/bips/commit/3c31af657fcfd7ed8c89b4ed6fc9ef2d11e58321"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:23:23Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:23:23Z"
      },
      "sha": "3ee99e66a93861b156a9bac2b9167b3acc5a2b82"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6YmE4MjMxNTcxMzU5MDczODIyNzVhZDA2M2U1NGY3NGExODc5YTM3Ng==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/ba823157135907382275ad063e54f74a1879a376",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/ba823157135907382275ad063e54f74a1879a376",
      "tree": {
        "sha": "239f955e3d1502a0b5faa7d5e81aeacea0d65158",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/239f955e3d1502a0b5faa7d5e81aeacea0d65158"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/3ee99e66a93861b156a9bac2b9167b3acc5a2b82",
          "sha": "3ee99e66a93861b156a9bac2b9167b3acc5a2b82",
          "html_url": "https://github.com/bitcoin/bips/commit/3ee99e66a93861b156a9bac2b9167b3acc5a2b82"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:25:11Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:25:11Z"
      },
      "sha": "ba823157135907382275ad063e54f74a1879a376"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6YmZlMzM2YzhlODg0NWFlNTViZGMwMmVjYTI4ZDkzZTFiMTFmYWRkZg==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/bfe336c8e8845ae55bdc02eca28d93e1b11faddf",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/bfe336c8e8845ae55bdc02eca28d93e1b11faddf",
      "tree": {
        "sha": "63ccf127b1e12ade59fdd16567f60375f21bb806",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/63ccf127b1e12ade59fdd16567f60375f21bb806"
      },
      "verification": {
        "verified": false,
        "reason": "unsigned",
        "payload": null,
        "signature": null
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/ba823157135907382275ad063e54f74a1879a376",
          "sha": "ba823157135907382275ad063e54f74a1879a376",
          "html_url": "https://github.com/bitcoin/bips/commit/ba823157135907382275ad063e54f74a1879a376"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:26:36Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:26:36Z"
      },
      "sha": "bfe336c8e8845ae55bdc02eca28d93e1b11faddf"
    },
    {
      "event": "committed",
      "id": null,
      "node_id": "MDY6Q29tbWl0MTQ1MzE3Mzc6ZDUyZjU4NmExMzA5YmUwNGUwMjk3ZTQ0ZmEwNmY2MjQxNzgwZTQ2Ng==",
      "url": "https://api.github.com/repos/bitcoin/bips/git/commits/d52f586a1309be04e0297e44fa06f6241780e466",
      "actor": null,
      "commit_id": null,
      "commit_url": null,
      "created_at": null,
      "html_url": "https://github.com/bitcoin/bips/commit/d52f586a1309be04e0297e44fa06f6241780e466",
      "tree": {
        "sha": "51926150a03f6c9f2bb6aa6354ca72fd196e1dba",
        "url": "https://api.github.com/repos/bitcoin/bips/git/trees/51926150a03f6c9f2bb6aa6354ca72fd196e1dba"
      },
      "verification": {
        "verified": true,
        "reason": "valid",
        "payload": "tree 51926150a03f6c9f2bb6aa6354ca72fd196e1dba\nparent bfe336c8e8845ae55bdc02eca28d93e1b11faddf\nauthor Olaoluwa Osuntokun <laolu32@gmail.com> 1512026906 -0800\ncommitter Olaoluwa Osuntokun <laolu32@gmail.com> 1512026911 -0800\n\nfixup! new bip: compact client side filtering\n",
        "signature": "-----BEGIN PGP SIGNATURE-----\n\niQIcBAABCgAGBQJaH7MlAAoJEJZOomPdY3wh0JAP/21Na27Qf5yILTx2xH08+UEG\nzhowyKio52MZjYBy7mWfy/VUk2vEi5Fc3FuVBDM0QkpPRAKn06spH53vebd2RvbH\ncTgCSAtp/tS2lBda/EjtY56+yUYOPrVAaQ2T2kW1t5P5TDRB1BsIQq9rxjyfGyX8\nZDMD9E6DQfpOy7WRhc9XL4fDQeeJLDhgNt9FN0+RPrzz0u3+Mik7njIAWqZKAd1x\nB3k2U87cqe9G+Ms9o2ZAOMN+Dh+rD/urbH5L25iptYZ9CBSW8LTyR9FKhVnqb0Cq\nUDKw9sI1NF/tmD7xbeihnjgcKwBFg/gGzfFLP5Hw9a1BTLnGKA17H34CM3FwaSZR\nJhFIPIvOK1Nqjy5//lk6xfc+mcm3yOzEvBLkqxtuOc5cajTVhtPWzlgvvalQHWTB\nh6Ks/yKZtd6wPiHBnFvklDDfpPz1JCEjDSYjfKyz1p0qItdUbYefdzR7EAGTpcS6\nuIcdXT0aRgxjWLvQFYHKbrUVtwyyqCpziU1XVcU7GQcshtS2U4iN8SIDFw2AYSdA\nTjKRJ8ySQoyVx29hrApwKK4jH4hoMUUP3XfUpsSrP/gqp5tzIzPIr+EWR/c7/jr0\n5Qh8kG1VO9J3+Ekyj8f7OQPfYlXsQPtHMYj7a6ayipXvi1RnzsxkIWjUilIrZBSS\n470rDHjR/7ufF15FqpTm\n=On+l\n-----END PGP SIGNATURE-----"
      },
      "parents": [
        {
          "url": "https://api.github.com/repos/bitcoin/bips/git/commits/bfe336c8e8845ae55bdc02eca28d93e1b11faddf",
          "sha": "bfe336c8e8845ae55bdc02eca28d93e1b11faddf",
          "html_url": "https://github.com/bitcoin/bips/commit/bfe336c8e8845ae55bdc02eca28d93e1b11faddf"
        }
      ],
      "message": "fixup! new bip: compact client side filtering",
      "committer": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:28:31Z"
      },
      "author": {
        "name": "Olaoluwa Osuntokun",
        "email": "laolu32@gmail.com",
        "date": "2017-11-30T07:28:26Z"
      },
      "sha": "d52f586a1309be04e0297e44fa06f6241780e466"
    },
    {
      "event": "commented",
      "id": 348105607,
      "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODEwNTYwNw==",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/comments/348105607",
      "actor": {
        "login": "evoskuil",
        "id": 1369335,
        "node_id": "MDQ6VXNlcjEzNjkzMzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1369335?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/evoskuil",
        "html_url": "https://github.com/evoskuil",
        "followers_url": "https://api.github.com/users/evoskuil/followers",
        "following_url": "https://api.github.com/users/evoskuil/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/evoskuil/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/evoskuil/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/evoskuil/subscriptions",
        "organizations_url": "https://api.github.com/users/evoskuil/orgs",
        "repos_url": "https://api.github.com/users/evoskuil/repos",
        "events_url": "https://api.github.com/users/evoskuil/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/evoskuil/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2017-11-30T07:30:43Z",
      "updated_at": "2017-11-30T07:32:24Z",
      "author_association": "CONTRIBUTOR",
      "body": "> The light client mode described in this BIP is meant to supersede BIP 37 as it provides a greater degree of privacy...\r\n\r\nAs far as I can tell the privacy improvement is never quantified. It would seem that the independent downloading of less than all blocks (which is ultimately the objective) is the only source of privacy loss. However, given that privacy is a primary objective, it would be helpful if this analysis was explicit. I am specifically interested in whether there is *any* privacy loss in the filtering aspect (as compared to the operation of a full node).",
      "user": {
        "login": "evoskuil",
        "id": 1369335,
        "node_id": "MDQ6VXNlcjEzNjkzMzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1369335?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/evoskuil",
        "html_url": "https://github.com/evoskuil",
        "followers_url": "https://api.github.com/users/evoskuil/followers",
        "following_url": "https://api.github.com/users/evoskuil/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/evoskuil/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/evoskuil/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/evoskuil/subscriptions",
        "organizations_url": "https://api.github.com/users/evoskuil/orgs",
        "repos_url": "https://api.github.com/users/evoskuil/repos",
        "events_url": "https://api.github.com/users/evoskuil/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/evoskuil/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bips/pull/609#issuecomment-348105607",
      "issue_url": "https://api.github.com/repos/bitcoin/bips/issues/609"
    },
    {
      "event": "commented",
      "id": 348277601,
      "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODI3NzYwMQ==",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/comments/348277601",
      "actor": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2017-11-30T18:27:55Z",
      "updated_at": "2017-11-30T18:27:55Z",
      "author_association": "MEMBER",
      "body": "@Roasbeef I see you're doing a lot of revising of stuff. Let me know when you'd like this merged. (others commenting should note that this is just a draft, and merging does not preclude acting on comments in future revisions)\r\n\r\nThere's no hard rule against test vectors in the repo. My opinion on where they belong is an opinion.",
      "user": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bips/pull/609#issuecomment-348277601",
      "issue_url": "https://api.github.com/repos/bitcoin/bips/issues/609"
    },
    {
      "event": "mentioned",
      "id": 1366237226,
      "node_id": "MDE0Ok1lbnRpb25lZEV2ZW50MTM2NjIzNzIyNg==",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/events/1366237226",
      "actor": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2017-11-30T18:27:55Z"
    },
    {
      "event": "subscribed",
      "id": 1366237227,
      "node_id": "MDE1OlN1YnNjcmliZWRFdmVudDEzNjYyMzcyMjc=",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/events/1366237227",
      "actor": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2017-11-30T18:27:55Z"
    },
    {
      "event": "reviewed",
      "id": 80257286,
      "node_id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3ODAyNTcyODY=",
      "url": null,
      "actor": null,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "body": "Just reviewed the P2P part again.",
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bips/pull/609#pullrequestreview-80257286",
      "submitted_at": "2017-11-30T18:39:46Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bips/pulls/609"
    },
    {
      "event": "reviewed",
      "id": 80369799,
      "node_id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3ODAzNjk3OTk=",
      "url": null,
      "actor": null,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "commit_url": null,
      "created_at": null,
      "author_association": "MEMBER",
      "user": {
        "login": "jamesob",
        "id": 73197,
        "node_id": "MDQ6VXNlcjczMTk3",
        "avatar_url": "https://avatars.githubusercontent.com/u/73197?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jamesob",
        "html_url": "https://github.com/jamesob",
        "followers_url": "https://api.github.com/users/jamesob/followers",
        "following_url": "https://api.github.com/users/jamesob/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jamesob/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jamesob/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jamesob/subscriptions",
        "organizations_url": "https://api.github.com/users/jamesob/orgs",
        "repos_url": "https://api.github.com/users/jamesob/repos",
        "events_url": "https://api.github.com/users/jamesob/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jamesob/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bips/pull/609#pullrequestreview-80369799",
      "submitted_at": "2017-11-30T23:52:50Z",
      "state": "COMMENTED",
      "pull_request_url": "https://api.github.com/repos/bitcoin/bips/pulls/609"
    },
    {
      "event": "commented",
      "id": 358470020,
      "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODQ3MDAyMA==",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/comments/358470020",
      "actor": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2018-01-17T22:26:37Z",
      "updated_at": "2018-01-17T22:26:37Z",
      "author_association": "CONTRIBUTOR",
      "body": "Closing this in favor of #636. \r\n\r\nThanks to everyone who gave valuable review comments, especially @TheBlueMatt for his comments on the P2P aspect which led to notable improvements as proposed by @jimpo. ",
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/bitcoin/bips/pull/609#issuecomment-358470020",
      "issue_url": "https://api.github.com/repos/bitcoin/bips/issues/609"
    },
    {
      "event": "mentioned",
      "id": 1429295429,
      "node_id": "MDE0Ok1lbnRpb25lZEV2ZW50MTQyOTI5NTQyOQ==",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/events/1429295429",
      "actor": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2018-01-17T22:26:37Z"
    },
    {
      "event": "subscribed",
      "id": 1429295432,
      "node_id": "MDE1OlN1YnNjcmliZWRFdmVudDE0MjkyOTU0MzI=",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/events/1429295432",
      "actor": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2018-01-17T22:26:37Z"
    },
    {
      "event": "mentioned",
      "id": 1429295434,
      "node_id": "MDE0Ok1lbnRpb25lZEV2ZW50MTQyOTI5NTQzNA==",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/events/1429295434",
      "actor": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2018-01-17T22:26:37Z"
    },
    {
      "event": "subscribed",
      "id": 1429295435,
      "node_id": "MDE1OlN1YnNjcmliZWRFdmVudDE0MjkyOTU0MzU=",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/events/1429295435",
      "actor": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2018-01-17T22:26:37Z"
    },
    {
      "event": "closed",
      "id": 1429300797,
      "node_id": "MDExOkNsb3NlZEV2ZW50MTQyOTMwMDc5Nw==",
      "url": "https://api.github.com/repos/bitcoin/bips/issues/events/1429300797",
      "actor": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "commit_id": null,
      "commit_url": null,
      "created_at": "2018-01-17T22:30:00Z"
    }
  ],
  "comments": [
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150177922",
      "pull_request_review_id": 75690121,
      "id": 150177922,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDE3NzkyMg==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 11,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Please choose one of the [recommended, or at least acceptable licenses](https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki#Recommended_licenses) (and also add a Copyright section below).",
      "created_at": "2017-11-10T08:38:13Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150177922",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150177922"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 11,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150178236",
      "pull_request_review_id": 75690121,
      "id": 150178236,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDE3ODIzNg==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 22,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "`[1]` should be `<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>` (and use `<references/>` below) to make it auto-linkable. (suggestion, not mandatory)",
      "created_at": "2017-11-10T08:39:49Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150178236",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150178236"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 22,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150380454",
      "pull_request_review_id": 75928460,
      "id": 150380454,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDM4MDQ1NA==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: ",
      "path": "gcs_light_client.mediawiki",
      "position": 931,
      "original_position": 157,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This should mention that k=4 in the example.\r\n\r\nAlso, I'm confused by the RLE scheme. How are runs of more than 2 1's represented? What would be the encoding of the below example with `s/0{20}/0{15} 1 0{5}/`?",
      "created_at": "2017-11-11T10:05:00Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150380454",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150380454"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 157,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150380493",
      "pull_request_review_id": 75928460,
      "id": 150380493,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDM4MDQ5Mw==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 97,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I feel like \"vector\" is more appropriate than \"array\" (not just in a C++ sense).",
      "created_at": "2017-11-11T10:07:22Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150380493",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150380493"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 97,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150616656",
      "pull_request_review_id": 75928460,
      "id": 150616656,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDYxNjY1Ng==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 169,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "typo: Due *to* the encoding",
      "created_at": "2017-11-13T17:57:29Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150616656",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150616656"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 169,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150617352",
      "pull_request_review_id": 75928460,
      "id": 150617352,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDYxNzM1Mg==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 201,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "typo: (q and *r*) for a given integer n\r\n\r\nAlso missing closing parenthesis.",
      "created_at": "2017-11-13T18:00:09Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150617352",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150617352"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 201,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150617926",
      "pull_request_review_id": 75928460,
      "id": 150617926,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDYxNzkyNg==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 239,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "typo: `aid`. aide is a noun.",
      "created_at": "2017-11-13T18:02:24Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150617926",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150617926"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 239,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150620174",
      "pull_request_review_id": 75928460,
      "id": 150620174,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDYyMDE3NA==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 297,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Doesn't mention what strategy this algorithm uses fewer cycles than.",
      "created_at": "2017-11-13T18:11:41Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150620174",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150620174"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 297,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150621564",
      "pull_request_review_id": 75928460,
      "id": 150621564,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDYyMTU2NA==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp",
      "path": "gcs_light_client.mediawiki",
      "position": 268,
      "original_position": 344,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Is it necessary to subtract the remainder? Just the shift is sufficient, no?",
      "created_at": "2017-11-13T18:17:30Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150621564",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150621564"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 268,
      "original_line": 344,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150622467",
      "pull_request_review_id": 75928460,
      "id": 150622467,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDYyMjQ2Nw==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 486,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "SFNodeCF",
      "created_at": "2017-11-13T18:21:08Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150622467",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150622467"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 486,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150628822",
      "pull_request_review_id": 75928460,
      "id": 150628822,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDYyODgyMg==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>",
      "path": "gcs_light_client.mediawiki",
      "position": 546,
      "original_position": 607,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Mentioning the command string seems redundant.",
      "created_at": "2017-11-13T18:45:06Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150628822",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150628822"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 546,
      "original_line": 607,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150639633",
      "pull_request_review_id": 76216067,
      "id": 150639633,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDYzOTYzMw==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: ",
      "path": "gcs_light_client.mediawiki",
      "position": 931,
      "original_position": 157,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150380454,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Yea, this is underspecified and also probably not entirely relevant here - the point of the Specification section is to describe *what* you do, not some alternative ways to encode the data.",
      "created_at": "2017-11-13T19:24:42Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150639633",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150639633"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 157,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150640522",
      "pull_request_review_id": 76216067,
      "id": 150640522,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY0MDUyMg==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).",
      "path": "gcs_light_client.mediawiki",
      "position": 958,
      "original_position": 184,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This paragraph probably doesn't belong in the \"Specification\" section, motivation section seems much more appropriate, or maybe a \"Background Math\" section.",
      "created_at": "2017-11-13T19:27:56Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150640522",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150640522"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 184,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150642260",
      "pull_request_review_id": 76216067,
      "id": 150642260,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY0MjI2MA==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.",
      "path": "gcs_light_client.mediawiki",
      "position": 426,
      "original_position": 502,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I dont think \"public key script\" is the way we normally refer to it....I'd say stick with either \"scriptPubKey\" or \"the output script\".",
      "created_at": "2017-11-13T19:34:36Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150642260",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150642260"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 426,
      "original_line": 502,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150643321",
      "pull_request_review_id": 76216067,
      "id": 150643321,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY0MzMyMQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and",
      "path": "gcs_light_client.mediawiki",
      "position": 123,
      "original_position": 115,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This is technically underspecified - \"unary\" is not a normative encoding, and can be 0-indexed, 1-indexed and can encode as 0s followed by a 1 or 1s followed by a 0.",
      "created_at": "2017-11-13T19:38:51Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150643321",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150643321"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 123,
      "original_line": 115,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150653135",
      "pull_request_review_id": 76216067,
      "id": 150653135,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY1MzEzNQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 330,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Wait, sorted_set? No where else does anything reference that the set must be sorted? Sorted in what order?",
      "created_at": "2017-11-13T20:17:58Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150653135",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150653135"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 330,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150654257",
      "pull_request_review_id": 76216067,
      "id": 150654257,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY1NDI1Nw==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)",
      "path": "gcs_light_client.mediawiki",
      "position": 477,
      "original_position": 540,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "extract_push_datas is never well-defined - only referenced in a general sense.",
      "created_at": "2017-11-13T20:22:28Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150654257",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150654257"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 477,
      "original_line": 540,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150654551",
      "pull_request_review_id": 76216067,
      "id": 150654551,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY1NDU1MQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()",
      "path": "gcs_light_client.mediawiki",
      "position": 498,
      "original_position": 560,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Sort order is not defined.",
      "created_at": "2017-11-13T20:23:43Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150654551",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150654551"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 498,
      "original_line": 560,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150655609",
      "pull_request_review_id": 76216067,
      "id": 150655609,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY1NTYwOQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 633,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Wait, huh? I think you mean meaning filters are *not* validated by full-nodes and *not* committed to in blocks.",
      "created_at": "2017-11-13T20:28:27Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150655609",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150655609"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 633,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150656007",
      "pull_request_review_id": 76216067,
      "id": 150656007,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY1NjAwNw==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 633,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150655609,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "More importantly, dunno why you have to specify this? It is binding insofar as the protocol says it must be X, and light clients will think you're broken if its not X.",
      "created_at": "2017-11-13T20:30:14Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150656007",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150656007"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 633,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150656915",
      "pull_request_review_id": 76216067,
      "id": 150656915,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY1NjkxNQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 648,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Soooo...0? Please just say 32 bytes of 0s.",
      "created_at": "2017-11-13T20:33:56Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150656915",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150656915"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 648,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150657179",
      "pull_request_review_id": 76216067,
      "id": 150657179,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY1NzE3OQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n):\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+      case None:",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 653,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Huh? Can you make it a bit clearer that you're saying if the filter has data in it? case Some, case None reads as nonsense to me (and you say this before you describe it in text).",
      "created_at": "2017-11-13T20:35:09Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150657179",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150657179"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 653,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150657416",
      "pull_request_review_id": 76216067,
      "id": 150657416,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY1NzQxNg==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n):\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+      case None:",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 653,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150657179,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Also, why the special case here? Why not just sha256 of the empty string?",
      "created_at": "2017-11-13T20:36:12Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150657416",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150657416"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 653,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150659449",
      "pull_request_review_id": 76216067,
      "id": 150659449,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY1OTQ0OQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n):\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its public key script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+This filter header chain can be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP should also fetch all the ''filter headers'' from ''each'' of",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 671,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think this section may be clearer if you use RFC words (which you appear to already be doing) and be clear that you are. Also, what is the advantage of fetching the filter headerS from each peer? It would be less bandwidth and just as much cross-checking to only fetch the top block's filter header after you've synced the headers chain (which is probably a much better approach in any case as you can first make sure you're on the best chain, and then download a filters chain, protecting you from some types of DoS issues). You can then do a backwards sync of the filters. If there is a mismatch you do a binary search or otherwise to figure out which peer is lying.",
      "created_at": "2017-11-13T20:45:05Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150659449",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150659449"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 671,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150659804",
      "pull_request_review_id": 76216067,
      "id": 150659804,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY1OTgwNA==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n):\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its public key script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+This filter header chain can be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP should also fetch all the ''filter headers'' from ''each'' of\n+their connected peers. This allows light clients to efficiently detect nodes\n+that advertise a conflicting filter chain history (by ensuring all nodes return\n+the ''same'' filter header hash for a particular block hash).\n+\n+Additionally, the filter header chain also allows a light client to efficiently\n+verify purported filter authenticity when fetching the next set of headers from\n+chain tip. Instead of fetching the filter ''from each peer'' (which wastes\n+bandwidth), a light client instead does the following:\n+\n+<pre>\n+verify_from_tip(tip_block_hash: [32]byte):",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 682,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I dont know why this needs to be in the normative \"Specification\" section - clients can use this how they feel, and I think this is probably not the best approach anyway. You may wish to say something like clients SHOULD avoid syncing the full filter chain from all peers, but instead should only download from multiple peers when an inconsistency is detected.",
      "created_at": "2017-11-13T20:46:46Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150659804",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150659804"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 682,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150659979",
      "pull_request_review_id": 76216067,
      "id": 150659979,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY1OTk3OQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n):\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its public key script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+This filter header chain can be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP should also fetch all the ''filter headers'' from ''each'' of\n+their connected peers. This allows light clients to efficiently detect nodes\n+that advertise a conflicting filter chain history (by ensuring all nodes return\n+the ''same'' filter header hash for a particular block hash).\n+\n+Additionally, the filter header chain also allows a light client to efficiently\n+verify purported filter authenticity when fetching the next set of headers from\n+chain tip. Instead of fetching the filter ''from each peer'' (which wastes\n+bandwidth), a light client instead does the following:\n+\n+<pre>\n+verify_from_tip(tip_block_hash: [32]byte):\n+    let filter_types = {supported_filter_types...}\n+    let connected_peers = {list_of_connected_full_nodes...}\n+\n+    for filter_type in filter_types:\n+\n+        let filter_headers = set()\n+        for peer in connected_peers:\n+            let filter_header = peer.fetch_filter_header(tip_block_hash)\n+            filter_headers.insert(filter_header)\n+\n+        if len(filter_headers) != 1:\n+            // Peers have conflicting filters. The light client should fetch\n+            // each unique filter from the set of peers AND fetch the block. The\n+            // light client can then verify which filter header is correct, and\n+            // BAN the offending peers.\n+\n+        // Otherwise, syncing continues as normal: fetch filter to see if it\n+        // matches any relevant items.\n+</pre>\n+\n+Light clients should persistently commit all filter headers to disk, as when",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 703,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I dont think this should be normative, but if it is, I'd suggest using RFC words and use MAY here instead of SHOULD.",
      "created_at": "2017-11-13T20:47:36Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150659979",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150659979"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 703,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150661111",
      "pull_request_review_id": 76216067,
      "id": 150661111,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY2MTExMQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n):\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its public key script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+This filter header chain can be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP should also fetch all the ''filter headers'' from ''each'' of\n+their connected peers. This allows light clients to efficiently detect nodes\n+that advertise a conflicting filter chain history (by ensuring all nodes return\n+the ''same'' filter header hash for a particular block hash).\n+\n+Additionally, the filter header chain also allows a light client to efficiently\n+verify purported filter authenticity when fetching the next set of headers from\n+chain tip. Instead of fetching the filter ''from each peer'' (which wastes\n+bandwidth), a light client instead does the following:\n+\n+<pre>\n+verify_from_tip(tip_block_hash: [32]byte):\n+    let filter_types = {supported_filter_types...}\n+    let connected_peers = {list_of_connected_full_nodes...}\n+\n+    for filter_type in filter_types:\n+\n+        let filter_headers = set()\n+        for peer in connected_peers:\n+            let filter_header = peer.fetch_filter_header(tip_block_hash)\n+            filter_headers.insert(filter_header)\n+\n+        if len(filter_headers) != 1:\n+            // Peers have conflicting filters. The light client should fetch\n+            // each unique filter from the set of peers AND fetch the block. The\n+            // light client can then verify which filter header is correct, and\n+            // BAN the offending peers.\n+\n+        // Otherwise, syncing continues as normal: fetch filter to see if it\n+        // matches any relevant items.\n+</pre>\n+\n+Light clients should persistently commit all filter headers to disk, as when\n+lazily fetching filters (due to a historical re-scan or chain analysis),\n+they're able to verify the authenticity of any fetched filters.\n+\n+Full-nodes should also persistently compute and persist the filter header chain\n+on-disk, just as the regular filters.\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages.\n+\n+The <code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can use these two messages to request a compact\n+filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message is defined as follows:\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the filter is being returned.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter being returned.\n+|-\n+| Var-Int\n+| NumFilterBytes\n+| uint64\n+| A variable length integer encoding the number of bytes of the filter in the following field.\n+|-\n+| NumFilterBytes\n+| FilterBytes\n+| [NumFilterBytes]byte\n+| The raw compressed compact filter for this block.\n+|}\n+\n+The <code>BlockHash</code> field is included in both messages as this allows\n+easily matching requests against responses, as the responses aren't\n+self-identifying like block headers are (via own hash).\n+\n+The parameters <code>N</code> (the number of elements in the filter) and\n+<code>P</code> (<code>1 << false_positive_rate</code>) are required by the light\n+client in order to properly incrementally decode, query, and validate\n+(reconstruct from Bitcoin block) a compact filter. The parameter <code>N</code>\n+cannot be known ahead of time, therefore we define the serialization of a\n+compact filter of type <code>0x00</code> and <code>0x01</code> as:\n+<pre>\n+N || raw_filter_bytes\n+</pre>\n+where <code>N</code> is serialized as a 32-bit big-endian integer.\n+\n+However, there exists a special case of a <code>null</code> filter. This this\n+case an empty byte slice is transmitted rather than consuming\n+<code>4-bytes</code> to encode the size of zero.\n+\n+However, as the parameter <code>P</code> ''must'' be globally agreed upon (for a\n+particular filter type), we define this value ''statically'' for filter types:\n+<code>0x00</code> and <code>0x01</code>. For the two aforementioned filter types,\n+the false positive rate has been chosen to be: <code>20</code>, meaning the\n+parameter <code>P</code> is: <code>2^20</code>, meaning <code>fp=20</code>.\n+This value was chosen as during simulations it was the value that minimized the\n+bandwidth utilized by the expected number of blocks downloaded due to false\n+positives, and the bandwidth used to download the filters themselves. The code along with a demo used for the paramter tuning can be found [here]\n+\n+\n+=== Protocol Version Bump ===",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 857,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I do not believe this is neccessary? There is a new service bit, which should be sufficient.",
      "created_at": "2017-11-13T20:52:04Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150661111",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150661111"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 857,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150662137",
      "pull_request_review_id": 76216067,
      "id": 150662137,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY2MjEzNw==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,",
      "path": "gcs_light_client.mediawiki",
      "position": 122,
      "original_position": 114,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think we need to much more seriously consider whether we want to, for the first time, introduce big endian at the purely-p2p layer - it is nice at a bit level, but it kinda sucks to have yet *more* endianness confusion in Bitcoin's P2P protocol.",
      "created_at": "2017-11-13T20:56:20Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r150662137",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/150662137"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 122,
      "original_line": 114,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/151209279",
      "pull_request_review_id": 76880119,
      "id": 151209279,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MTIwOTI3OQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 296,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": null,
      "user": {
        "login": "jimpo",
        "id": 881253,
        "node_id": "MDQ6VXNlcjg4MTI1Mw==",
        "avatar_url": "https://avatars.githubusercontent.com/u/881253?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jimpo",
        "html_url": "https://github.com/jimpo",
        "followers_url": "https://api.github.com/users/jimpo/followers",
        "following_url": "https://api.github.com/users/jimpo/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jimpo/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jimpo/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jimpo/subscriptions",
        "organizations_url": "https://api.github.com/users/jimpo/orgs",
        "repos_url": "https://api.github.com/users/jimpo/repos",
        "events_url": "https://api.github.com/users/jimpo/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jimpo/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "64 bits appears as a magic number here. I would note that the 64 bits comes from the SipHash output size.",
      "created_at": "2017-11-15T18:17:09Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r151209279",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/151209279"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 296,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999816",
      "pull_request_review_id": 80093704,
      "id": 153999816,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTgxNg==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 11,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150177922,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Dunzo, we chose `CCO-1.0`. ",
      "created_at": "2017-11-30T07:14:30Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999816",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999816"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 11,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999821",
      "pull_request_review_id": 80093709,
      "id": 153999821,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTgyMQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 22,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150178236,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Done. ",
      "created_at": "2017-11-30T07:14:32Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999821",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999821"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 22,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999823",
      "pull_request_review_id": 80093712,
      "id": 153999823,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTgyMw==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 97,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150380493,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Modified wording to say \"vector\" instead of array. ",
      "created_at": "2017-11-30T07:14:34Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999823",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999823"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 97,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999827",
      "pull_request_review_id": 80093717,
      "id": 153999827,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTgyNw==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: ",
      "path": "gcs_light_client.mediawiki",
      "position": 931,
      "original_position": 157,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150380454,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Opted to move this to a \"Background\" appendix at the end of the document. ",
      "created_at": "2017-11-30T07:14:36Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999827",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999827"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 157,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999829",
      "pull_request_review_id": 80093720,
      "id": 153999829,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTgyOQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 201,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150617352,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed. ",
      "created_at": "2017-11-30T07:14:38Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999829",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999829"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 201,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999831",
      "pull_request_review_id": 80093724,
      "id": 153999831,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTgzMQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 239,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150617926,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed. ",
      "created_at": "2017-11-30T07:14:40Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999831",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999831"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 239,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999836",
      "pull_request_review_id": 80093733,
      "id": 153999836,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTgzNg==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 297,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150620174,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Elaborated a bit more. ",
      "created_at": "2017-11-30T07:14:41Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999836",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999836"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 297,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999842",
      "pull_request_review_id": 80093742,
      "id": 153999842,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg0Mg==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 486,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150622467,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed. ",
      "created_at": "2017-11-30T07:14:43Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999842",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999842"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 486,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999845",
      "pull_request_review_id": 80093746,
      "id": 153999845,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg0NQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,",
      "path": "gcs_light_client.mediawiki",
      "position": 122,
      "original_position": 114,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150662137,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Big endian just seemed natural initially. It's not as if we're using big-endian to encode a field that was _previously_ encoded using little-endian. This is a new data-structure all together. ",
      "created_at": "2017-11-30T07:14:45Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999845",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999845"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 122,
      "original_line": 114,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999849",
      "pull_request_review_id": 80093754,
      "id": 153999849,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg0OQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and",
      "path": "gcs_light_client.mediawiki",
      "position": 123,
      "original_position": 115,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150643321,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Updated to make more explicit. ",
      "created_at": "2017-11-30T07:14:47Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999849",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999849"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 123,
      "original_line": 115,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999854",
      "pull_request_review_id": 80093758,
      "id": 153999854,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg1NA==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).",
      "path": "gcs_light_client.mediawiki",
      "position": 958,
      "original_position": 184,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150640522,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Great suggestion, move the bulk of the initial section to a \"Background Math\" section at the end of the document. ",
      "created_at": "2017-11-30T07:14:49Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999854",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999854"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 184,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999859",
      "pull_request_review_id": 80093762,
      "id": 153999859,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg1OQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 330,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150653135,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "The `hashed_set_construct` function above this returns a sorted set. Update to point this out, and make it explicit in the section above `gcs_compress`. ",
      "created_at": "2017-11-30T07:14:50Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999859",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999859"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 330,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999862",
      "pull_request_review_id": 80093765,
      "id": 153999862,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg2Mg==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.",
      "path": "gcs_light_client.mediawiki",
      "position": 426,
      "original_position": 502,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150642260,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Modified to use \"output script\". ",
      "created_at": "2017-11-30T07:14:52Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999862",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999862"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 426,
      "original_line": 502,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999866",
      "pull_request_review_id": 80093768,
      "id": 153999866,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg2Ng==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)",
      "path": "gcs_light_client.mediawiki",
      "position": 477,
      "original_position": 540,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150654257,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Added a new section explaining what's meant by `extract_push_datas`. I've left a level of abstraction though, as otherwise I'd need to start specifying Script itself...so it currently assumes a certain level of familiarity with Bitcoin (as do all BIPs). ",
      "created_at": "2017-11-30T07:14:53Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999866",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999866"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 477,
      "original_line": 540,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999867",
      "pull_request_review_id": 80093770,
      "id": 153999867,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg2Nw==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()",
      "path": "gcs_light_client.mediawiki",
      "position": 498,
      "original_position": 560,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150654551,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Defined. ",
      "created_at": "2017-11-30T07:14:55Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999867",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999867"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 498,
      "original_line": 560,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999871",
      "pull_request_review_id": 80093773,
      "id": 153999871,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg3MQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 633,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150655609,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Ah yeh, nice catch! Modified to say \"not\". Specifying it to give the reader the rationale to why the header chain exists. ",
      "created_at": "2017-11-30T07:14:57Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999871",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999871"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 633,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999875",
      "pull_request_review_id": 80093780,
      "id": 153999875,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg3NQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 648,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150656915,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Fixed. ",
      "created_at": "2017-11-30T07:14:58Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999875",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999875"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 648,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999878",
      "pull_request_review_id": 80093784,
      "id": 153999878,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg3OA==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n):\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+      case None:",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 653,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150657179,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Added further text explaining the two cases. Also defined the whole `match`, `some`, `none` usage at the top in the preliminaries (borrowed from more functional languages). \r\n\r\nNo particular reason, but this way 32-byte are _always_ hashed current filter header. ",
      "created_at": "2017-11-30T07:15:00Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999878",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999878"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 653,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999881",
      "pull_request_review_id": 80093791,
      "id": 153999881,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg4MQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n):\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its public key script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+This filter header chain can be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP should also fetch all the ''filter headers'' from ''each'' of",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 671,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150659449,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Modified to make the usage of RFC works explicit. The section below assumes the client has fully synced, as is fetching from tip. As a result, they already have the entire header chain. The naive way would be just to fetch the _entire_ filter from each peer. This algo avoids that and only fetches the filter in the case a bamboozlement attempt by a peer.",
      "created_at": "2017-11-30T07:15:01Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999881",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999881"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 671,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999885",
      "pull_request_review_id": 80093797,
      "id": 153999885,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg4NQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n):\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its public key script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+This filter header chain can be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP should also fetch all the ''filter headers'' from ''each'' of\n+their connected peers. This allows light clients to efficiently detect nodes\n+that advertise a conflicting filter chain history (by ensuring all nodes return\n+the ''same'' filter header hash for a particular block hash).\n+\n+Additionally, the filter header chain also allows a light client to efficiently\n+verify purported filter authenticity when fetching the next set of headers from\n+chain tip. Instead of fetching the filter ''from each peer'' (which wastes\n+bandwidth), a light client instead does the following:\n+\n+<pre>\n+verify_from_tip(tip_block_hash: [32]byte):",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 682,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150659804,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Moved to the \"Implementation Notes\" section. ",
      "created_at": "2017-11-30T07:15:03Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999885",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999885"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 682,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999887",
      "pull_request_review_id": 80093801,
      "id": 153999887,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg4Nw==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n):\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its public key script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+This filter header chain can be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP should also fetch all the ''filter headers'' from ''each'' of\n+their connected peers. This allows light clients to efficiently detect nodes\n+that advertise a conflicting filter chain history (by ensuring all nodes return\n+the ''same'' filter header hash for a particular block hash).\n+\n+Additionally, the filter header chain also allows a light client to efficiently\n+verify purported filter authenticity when fetching the next set of headers from\n+chain tip. Instead of fetching the filter ''from each peer'' (which wastes\n+bandwidth), a light client instead does the following:\n+\n+<pre>\n+verify_from_tip(tip_block_hash: [32]byte):\n+    let filter_types = {supported_filter_types...}\n+    let connected_peers = {list_of_connected_full_nodes...}\n+\n+    for filter_type in filter_types:\n+\n+        let filter_headers = set()\n+        for peer in connected_peers:\n+            let filter_header = peer.fetch_filter_header(tip_block_hash)\n+            filter_headers.insert(filter_header)\n+\n+        if len(filter_headers) != 1:\n+            // Peers have conflicting filters. The light client should fetch\n+            // each unique filter from the set of peers AND fetch the block. The\n+            // light client can then verify which filter header is correct, and\n+            // BAN the offending peers.\n+\n+        // Otherwise, syncing continues as normal: fetch filter to see if it\n+        // matches any relevant items.\n+</pre>\n+\n+Light clients should persistently commit all filter headers to disk, as when",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 703,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150659979,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Modified to use MAY. ",
      "created_at": "2017-11-30T07:15:04Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999887",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999887"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 703,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999891",
      "pull_request_review_id": 80093805,
      "id": 153999891,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTg5MQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP are to treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types should be constructed,\n+and stored on disk. Full-nodes that update to support this BIP once already\n+synced, should upon start-up, re-index the chain, constructing filters for each\n+block from genesis to current chain tip.\n+\n+Given a Bitcoin block, a full-node constructs a <code>Normal</code> compact\n+filter as follows:\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            raw_items.append(output_bytes)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+\n+Given a Bitcoin block, a full-node construct an <code>Extended</code> compact filter as follows\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message should respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is validated by full-nodes and committed into blocks by miners, we\n+require an alternative (albeit less-binding) method to allow light clients to\n+''identify'' and ''reject'' invalid filters. The purely p2p solution to this\n+problem is to obtain a deterministic hash-chain of ''each'' filter. This hash\n+chain or \"filter header chain\" is similar to the regular Bitcoin headers in\n+that it allows a light client to verify the ''authenticity'' of a received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n):\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its public key script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+This filter header chain can be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP should also fetch all the ''filter headers'' from ''each'' of\n+their connected peers. This allows light clients to efficiently detect nodes\n+that advertise a conflicting filter chain history (by ensuring all nodes return\n+the ''same'' filter header hash for a particular block hash).\n+\n+Additionally, the filter header chain also allows a light client to efficiently\n+verify purported filter authenticity when fetching the next set of headers from\n+chain tip. Instead of fetching the filter ''from each peer'' (which wastes\n+bandwidth), a light client instead does the following:\n+\n+<pre>\n+verify_from_tip(tip_block_hash: [32]byte):\n+    let filter_types = {supported_filter_types...}\n+    let connected_peers = {list_of_connected_full_nodes...}\n+\n+    for filter_type in filter_types:\n+\n+        let filter_headers = set()\n+        for peer in connected_peers:\n+            let filter_header = peer.fetch_filter_header(tip_block_hash)\n+            filter_headers.insert(filter_header)\n+\n+        if len(filter_headers) != 1:\n+            // Peers have conflicting filters. The light client should fetch\n+            // each unique filter from the set of peers AND fetch the block. The\n+            // light client can then verify which filter header is correct, and\n+            // BAN the offending peers.\n+\n+        // Otherwise, syncing continues as normal: fetch filter to see if it\n+        // matches any relevant items.\n+</pre>\n+\n+Light clients should persistently commit all filter headers to disk, as when\n+lazily fetching filters (due to a historical re-scan or chain analysis),\n+they're able to verify the authenticity of any fetched filters.\n+\n+Full-nodes should also persistently compute and persist the filter header chain\n+on-disk, just as the regular filters.\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages.\n+\n+The <code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can use these two messages to request a compact\n+filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message is defined as follows:\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the filter is being returned.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter being returned.\n+|-\n+| Var-Int\n+| NumFilterBytes\n+| uint64\n+| A variable length integer encoding the number of bytes of the filter in the following field.\n+|-\n+| NumFilterBytes\n+| FilterBytes\n+| [NumFilterBytes]byte\n+| The raw compressed compact filter for this block.\n+|}\n+\n+The <code>BlockHash</code> field is included in both messages as this allows\n+easily matching requests against responses, as the responses aren't\n+self-identifying like block headers are (via own hash).\n+\n+The parameters <code>N</code> (the number of elements in the filter) and\n+<code>P</code> (<code>1 << false_positive_rate</code>) are required by the light\n+client in order to properly incrementally decode, query, and validate\n+(reconstruct from Bitcoin block) a compact filter. The parameter <code>N</code>\n+cannot be known ahead of time, therefore we define the serialization of a\n+compact filter of type <code>0x00</code> and <code>0x01</code> as:\n+<pre>\n+N || raw_filter_bytes\n+</pre>\n+where <code>N</code> is serialized as a 32-bit big-endian integer.\n+\n+However, there exists a special case of a <code>null</code> filter. This this\n+case an empty byte slice is transmitted rather than consuming\n+<code>4-bytes</code> to encode the size of zero.\n+\n+However, as the parameter <code>P</code> ''must'' be globally agreed upon (for a\n+particular filter type), we define this value ''statically'' for filter types:\n+<code>0x00</code> and <code>0x01</code>. For the two aforementioned filter types,\n+the false positive rate has been chosen to be: <code>20</code>, meaning the\n+parameter <code>P</code> is: <code>2^20</code>, meaning <code>fp=20</code>.\n+This value was chosen as during simulations it was the value that minimized the\n+bandwidth utilized by the expected number of blocks downloaded due to false\n+positives, and the bandwidth used to download the filters themselves. The code along with a demo used for the paramter tuning can be found [here]\n+\n+\n+=== Protocol Version Bump ===",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 857,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150661111,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "AFAIK, all new extensions to the p2p protocol have been accompanied by a protocol version bump? (or at least ones that add _new_ messages) ",
      "created_at": "2017-11-30T07:15:06Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999891",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999891"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 857,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999904",
      "pull_request_review_id": 80093819,
      "id": 153999904,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mzk5OTkwNA==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly",
      "path": "gcs_light_client.mediawiki",
      "position": null,
      "original_position": 296,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 151209279,
      "user": {
        "login": "Roasbeef",
        "id": 998190,
        "node_id": "MDQ6VXNlcjk5ODE5MA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/998190?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Roasbeef",
        "html_url": "https://github.com/Roasbeef",
        "followers_url": "https://api.github.com/users/Roasbeef/followers",
        "following_url": "https://api.github.com/users/Roasbeef/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/Roasbeef/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/Roasbeef/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/Roasbeef/subscriptions",
        "organizations_url": "https://api.github.com/users/Roasbeef/orgs",
        "repos_url": "https://api.github.com/users/Roasbeef/repos",
        "events_url": "https://api.github.com/users/Roasbeef/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/Roasbeef/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Specified. ",
      "created_at": "2017-11-30T07:15:11Z",
      "updated_at": "2017-11-30T07:28:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r153999904",
      "author_association": "CONTRIBUTOR",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/153999904"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": null,
      "original_line": 296,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154139546",
      "pull_request_review_id": 80257286,
      "id": 154139546,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDEzOTU0Ng==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the",
      "path": "gcs_light_client.mediawiki",
      "position": 405,
      "original_position": 405,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think SHOULD is maybe wrong here - some light clients will almost certainly have to *only* connect to nodes that have the bit set. You could replace with MAY preferentially, or only connect to, as required, or just drop it.",
      "created_at": "2017-11-30T17:05:18Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154139546",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154139546"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 405,
      "original_line": 405,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154139679",
      "pull_request_review_id": 80257286,
      "id": 154139679,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDEzOTY3OQ==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the",
      "path": "gcs_light_client.mediawiki",
      "position": 408,
      "original_position": 408,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I guess terminology...its kinda the 7th bit.",
      "created_at": "2017-11-30T17:05:47Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154139679",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154139679"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 408,
      "original_line": 408,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154140561",
      "pull_request_review_id": 80257286,
      "id": 154140561,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE0MDU2MQ==",
      "diff_hunk": "@@ -0,0 +1,992 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: PD\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP 37. The light client mode described in this BIP can be seen as\n+a \"reversal\"[1] of BIP 37: rather than the light clients sending filters to\n+full-nodes, full-nodes send filters to light clients. Unlike BIP 37, we don't\n+utilize bloom filters. Instead, we utilize a compact filter (more efficient\n+than bloom filters) which leverages Golomb-Rice coding for compression.\n+Additionally, blocks are downloaded as a whole (from any source), rather than\n+directly from peers as fragments with merkle-branches proving their\n+authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism [3][4]. Additionally, applications are forced to carefully manage\n+their false positive rates in order to not completely give away their set of\n+interested items. Additionally, full-nodes can nearly undetectably lie by\n+omission, causing a denial of service which can lead to undesirable failure modes\n+in applications whose safety critically relies on responding to certain\n+on-chain events. When faithfully servicing BIP 37 light clients, full-nodes may\n+incur significant I/O and CPU resource usage due to maliciously crafted bloom\n+filters, creating a denial-of-service vector.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval [5].\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or array) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding.\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks.\n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due\n+the the encoding of runs, RLE works best when encoding a set with a high degree\n+of redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>m</code> for a given integer <code>n</code> as a two-tuple. The first value\n+<code>q</code> is encoded using ''unary'', and the second value <code>r</code> is encoded using a\n+fixed-length series of bits. If <code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code> least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code> that divide\n+into <code>n</code> If an encoded integer is close to the value of <code>m</code> then few bits (in\n+unary) will be used to encode each value.\n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice coding. These functions will be used in the next section as a\n+primitive in the construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+To aide in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+    \n+<code>P</code> can also be interpreted as the parameter to our Geometric\n+Distribution.  Intuitively, to achieve a false positive rate of 1/32 (1/2^5),\n+in a series of queries of items which ''aren't'' in the set, we expect to\n+receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>\n+<code>F</code> constricts the range of the hashed values accordingly in order to\n+achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range algorithm[7],\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without division and can be done with fewer cycles\n+cycles on CPUs that have 128-bit registers.\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted) we then use\n+Golomb-Rice coding to ''compress'' the set by encoding the ''delta'' value\n+between each successive element within the set. As these values are uniformly\n+distributed, the deltas between these values will be Geometrically Distributed,\n+meaning that Golomb-Rice coding will be optimal for this use-case [6].\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one must\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one must first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients will need to ''preferentially'' peer to full-nodes that support\n+the features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>CFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.",
      "path": "gcs_light_client.mediawiki",
      "position": 426,
      "original_position": 502,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "83b83c78e189be898573e0bfe936dd0c9b99ecb9",
      "in_reply_to_id": 150642260,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "No?",
      "created_at": "2017-11-30T17:08:54Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154140561",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154140561"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 426,
      "original_line": 502,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154142490",
      "pull_request_review_id": 80257286,
      "id": 154142490,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE0MjQ5MA==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: ",
      "path": "gcs_light_client.mediawiki",
      "position": 409,
      "original_position": 409,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This is hugely under-defined - can you set the BIP if you're not a full node (ie is NODE_NETWORK implied or is that separate and you can look for eg NODE_NETWORK_LIMITED & NODE_CF)? Do you have to have implemented both currently-defined filter types (No is better for maintainability, but that has huge useability implications)? What does this bit mean without NODE_NETWORK? Maybe you always have to return the filter chain hashes but dont need to return the filters themselves?",
      "created_at": "2017-11-30T17:16:16Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154142490",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154142490"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 409,
      "original_line": 409,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154142669",
      "pull_request_review_id": 80257286,
      "id": 154142669,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE0MjY2OQ==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>",
      "path": "gcs_light_client.mediawiki",
      "position": 410,
      "original_position": 410,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "What is the SF prefix here? I think generally BIPs have used Bitcoin Core's NODE_* constant naming (and generally use all-caps constant naming).",
      "created_at": "2017-11-30T17:16:55Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154142669",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154142669"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 410,
      "original_line": 410,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154143597",
      "pull_request_review_id": 80257286,
      "id": 154143597,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE0MzU5Nw==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====",
      "path": "gcs_light_client.mediawiki",
      "position": 539,
      "original_position": 539,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I think this is kinda poorly thought-through. As noted in the service bit discussion section, its not clear to me how you'd actually *use* this in practice. Lets say a new filter type is added to include segwit/p2sh redeemScript pushes, now a client that wants that filter has to go around connecting to every node it can find with the service bit until it finds one that supports the new filter type?",
      "created_at": "2017-11-30T17:19:46Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154143597",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154143597"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 539,
      "original_line": 539,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154144120",
      "pull_request_review_id": 80257286,
      "id": 154144120,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE0NDEyMA==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can SHOULD use these two messages to request a\n+compact filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message MUST be sent in response to a\n+<code>getcfilter</code> message for a particular block hash.The\n+<code>cfilter</code> message is defined as follows:\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the filter is being returned.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter being returned.\n+|-\n+| Var-Int\n+| NumFilterBytes\n+| uint64\n+| A variable length integer encoding the number of bytes of the filter in the following field.\n+|-\n+| NumFilterBytes\n+| FilterBytes\n+| [NumFilterBytes]byte\n+| The raw compressed compact filter for this block.\n+|}\n+\n+The <code>BlockHash</code> field is included in both messages as this allows\n+easily matching requests against responses, as the responses aren't\n+self-identifying like block headers are (via own hash).\n+\n+The parameters <code>N</code> (the number of elements in the filter) and\n+<code>P</code> (<code>1 << false_positive_rate</code>) are required by the light\n+client in order to properly incrementally decode, query, and validate\n+(reconstruct from Bitcoin block) a compact filter. The parameter <code>N</code>\n+cannot be known ahead of time, therefore we define the serialization of a\n+compact filter of type <code>0x00</code> and <code>0x01</code> as:\n+<pre>\n+N || raw_filter_bytes\n+</pre>\n+where <code>N</code> is serialized as a 32-bit big-endian integer.\n+\n+However, there exists a special case of a <code>null</code> filter. This this\n+case an empty byte slice is transmitted rather than consuming\n+<code>4-bytes</code> to encode the size of zero.\n+\n+However, as the parameter <code>P</code> ''MUST'' be globally agreed upon (for\n+a particular filter type), we define this value ''statically'' for filter\n+types: <code>0x00</code> and <code>0x01</code>. For the two aforementioned\n+filter types, the false positive rate MUST be: <code>20</code>, meaning the\n+parameter <code>P</code> is: <code>2^20</code>, meaning <code>fp=20</code>.\n+This value was chosen as during simulations it was the value that minimized the\n+bandwidth utilized by the expected number of blocks downloaded due to false\n+positives, and the bandwidth used to download the filters themselves. The code\n+along with a demo used for the parameter tuning can be found [here https://github.com/Roasbeef/bips/blob/83b83c78e189be898573e0bfe936dd0c9b99ecb9/gcs_light_client/gentestvectors.go]\n+\n+\n+=== Protocol Version Bump ===\n+\n+As this BIP defines new peer-to-peer behavior, we bump the protocol version by",
      "path": "gcs_light_client.mediawiki",
      "position": 770,
      "original_position": 770,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "As noted previously (but github maybe lost the comment?), given the service bit inclusion, this is uneccessary. Would strongly prefer you *not* bump the protocol version.",
      "created_at": "2017-11-30T17:21:35Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154144120",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154144120"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 770,
      "original_line": 770,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154144380",
      "pull_request_review_id": 80257286,
      "id": 154144380,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE0NDM4MA==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can SHOULD use these two messages to request a\n+compact filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message MUST be sent in response to a\n+<code>getcfilter</code> message for a particular block hash.The\n+<code>cfilter</code> message is defined as follows:\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the filter is being returned.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter being returned.\n+|-\n+| Var-Int\n+| NumFilterBytes\n+| uint64\n+| A variable length integer encoding the number of bytes of the filter in the following field.\n+|-\n+| NumFilterBytes\n+| FilterBytes\n+| [NumFilterBytes]byte\n+| The raw compressed compact filter for this block.\n+|}\n+\n+The <code>BlockHash</code> field is included in both messages as this allows\n+easily matching requests against responses, as the responses aren't\n+self-identifying like block headers are (via own hash).\n+\n+The parameters <code>N</code> (the number of elements in the filter) and\n+<code>P</code> (<code>1 << false_positive_rate</code>) are required by the light\n+client in order to properly incrementally decode, query, and validate\n+(reconstruct from Bitcoin block) a compact filter. The parameter <code>N</code>\n+cannot be known ahead of time, therefore we define the serialization of a\n+compact filter of type <code>0x00</code> and <code>0x01</code> as:\n+<pre>\n+N || raw_filter_bytes\n+</pre>\n+where <code>N</code> is serialized as a 32-bit big-endian integer.\n+\n+However, there exists a special case of a <code>null</code> filter. This this\n+case an empty byte slice is transmitted rather than consuming\n+<code>4-bytes</code> to encode the size of zero.\n+\n+However, as the parameter <code>P</code> ''MUST'' be globally agreed upon (for\n+a particular filter type), we define this value ''statically'' for filter\n+types: <code>0x00</code> and <code>0x01</code>. For the two aforementioned\n+filter types, the false positive rate MUST be: <code>20</code>, meaning the\n+parameter <code>P</code> is: <code>2^20</code>, meaning <code>fp=20</code>.\n+This value was chosen as during simulations it was the value that minimized the\n+bandwidth utilized by the expected number of blocks downloaded due to false\n+positives, and the bandwidth used to download the filters themselves. The code\n+along with a demo used for the parameter tuning can be found [here https://github.com/Roasbeef/bips/blob/83b83c78e189be898573e0bfe936dd0c9b99ecb9/gcs_light_client/gentestvectors.go]\n+\n+\n+=== Protocol Version Bump ===\n+\n+As this BIP defines new peer-to-peer behavior, we bump the protocol version by\n+one in order to distinguish the newly defined behavior. Full-nodes implementing\n+this BIP should advertise a protocol version of: <code>70016</code>.\n+\n+== New Wallet Capabilities Enabled ==\n+\n+The new light client mode enables wallet to maintain a very compact client-side\n+index of (possibly) the entire chain. Such an index provides a great degree of\n+utility for wallets, as they're now able to perform tasks such as private key\n+imports and full HD-seed imports without the need of a trusted third-party\n+server. Additionally, the compact client-side chain index also opens up the\n+door to smart contract applications which require agent action in response to\n+on-chain events. Examples of such applications include Lightning.\n+\n+== Backwards Compatability ==\n+\n+This light client protocl is NOT backwards compatible with BIP 37. Full nodes\n+MAY implement both protocols to serve both types of light clients.\n+\n+\n+== Implementation Notes ==\n+\n+This filter header chain SHOULD be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP SHOULD also fetch all the ''filter headers'' from\n+''each'' of their connected peers. With these headers, light clients SHOULD\n+efficiently detect nodes that advertise a conflicting filter chain history. To\n+do this, light clients MUST ensure that all nodes return the ``same` filter\n+header hash for a particular block header hash. \n+\n+Light clients MAY use the filter header chain to verify purported filter\n+authenticity when fetching the next set of headers from chain tip. Light\n+clients MAY use the following algorithm to more efficiently verify the\n+authenticity of filters (the naive version would fetch the entire filter from\n+each peer, this version saves bandwidth):\n+\n+<pre>\n+verify_from_tip(tip_block_hash: [32]byte):\n+    let filter_types = {supported_filter_types...}\n+    let connected_peers = {list_of_connected_full_nodes...}\n+\n+    for filter_type in filter_types:\n+\n+        let filter_headers = set()\n+        for peer in connected_peers:\n+            let filter_header = peer.fetch_filter_header(tip_block_hash)\n+            filter_headers.insert(filter_header)\n+\n+        if len(filter_headers) != 1:\n+            // Peers have conflicting filters. The light client should fetch\n+            // each unique filter from the set of peers AND fetch the block. The\n+            // light client can then verify which filter header is correct, and\n+            // BAN the offending peers.\n+\n+        // Otherwise, syncing continues as normal: fetch filter to see if it\n+        // matches any relevant items.\n+</pre>\n+\n+Light clients MAY persistently commit all filter headers to disk, as when\n+lazily fetching filters (due to a historical re-scan or chain analysis),\n+they're able to verify the authenticity of any fetched filters.\n+\n+Full-nodes MAY persistently compute and persist the filter header chain\n+on-disk, just as the regular filters.\n+\n+Zero-length filters are sent without an <code>N</code> value, allowing us to save\n+<code>4-bytes</code>. Clients are able to verify that a filter will be\n+<code>null</code> before requesting it (as it will just be the prior filter\n+header hashed with zero bytes). Clients can take this fact into account in\n+order to save a round trip for <code>null</code> headers.\n+\n+Light clients implementing this proposal SHOULD utilize the\n+<code>sendheaders</code> message. This allow quicker syncing from tip, as rather\n+than sending an <code>inv</code> to announce a new blocks, nodes will instead\n+directly send the <code>headers</code> message. With this, nodes can save a\n+round-trip and immediately request\n+<code>cfheaders</code> from each of their connected peers before ultimately\n+fetching the filter itself.\n+\n+Light client syncing MAY be run in reverse, meaning fetching the regular\n+Bitcoin and filter headers from the ''end'' of the chain and working backwards.\n+This allows a client to nearly ''instantly'' be useable if an application doesn't\n+require immediate access to historical filter data.\n+\n+If fetching blocks directly over the p2p network (rather than via a distinct\n+channel), light clients SHOULD fetch blocks from multiple peers in order to\n+mitigate transaction intersection analysis.\n+\n+Full nodes serving the light clients described in this BIP can be implemented\n+in a completely ''stateless'' manner. Implementations can also make a\n+space/time tradeoff by only holding filters on disk to some particular block\n+horizon. If a filter beyond this horizon is requested, then it can be\n+reconstructed in real time. This allows nodes to save on disk-space, as it's\n+likely the older filters would only be fetched for historical rescans. It is\n+also likely that the past few hundred filters will be fetched mostly frequently\n+by smaller devices (phones, laptops, etc) periodically coming back online after a\n+period of inactivity.\n+\n+It is possible to implementations of this BIP to serve ''other''",
      "path": "gcs_light_client.mediawiki",
      "position": 869,
      "original_position": 869,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I have no idea what this sentence is saying?",
      "created_at": "2017-11-30T17:22:34Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154144380",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154144380"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 869,
      "original_line": 869,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154144526",
      "pull_request_review_id": 80257286,
      "id": 154144526,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE0NDUyNg==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can SHOULD use these two messages to request a\n+compact filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message MUST be sent in response to a\n+<code>getcfilter</code> message for a particular block hash.The\n+<code>cfilter</code> message is defined as follows:\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the filter is being returned.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter being returned.\n+|-\n+| Var-Int\n+| NumFilterBytes\n+| uint64\n+| A variable length integer encoding the number of bytes of the filter in the following field.\n+|-\n+| NumFilterBytes\n+| FilterBytes\n+| [NumFilterBytes]byte\n+| The raw compressed compact filter for this block.\n+|}\n+\n+The <code>BlockHash</code> field is included in both messages as this allows\n+easily matching requests against responses, as the responses aren't\n+self-identifying like block headers are (via own hash).\n+\n+The parameters <code>N</code> (the number of elements in the filter) and\n+<code>P</code> (<code>1 << false_positive_rate</code>) are required by the light\n+client in order to properly incrementally decode, query, and validate\n+(reconstruct from Bitcoin block) a compact filter. The parameter <code>N</code>\n+cannot be known ahead of time, therefore we define the serialization of a\n+compact filter of type <code>0x00</code> and <code>0x01</code> as:\n+<pre>\n+N || raw_filter_bytes\n+</pre>\n+where <code>N</code> is serialized as a 32-bit big-endian integer.\n+\n+However, there exists a special case of a <code>null</code> filter. This this\n+case an empty byte slice is transmitted rather than consuming\n+<code>4-bytes</code> to encode the size of zero.\n+\n+However, as the parameter <code>P</code> ''MUST'' be globally agreed upon (for\n+a particular filter type), we define this value ''statically'' for filter\n+types: <code>0x00</code> and <code>0x01</code>. For the two aforementioned\n+filter types, the false positive rate MUST be: <code>20</code>, meaning the\n+parameter <code>P</code> is: <code>2^20</code>, meaning <code>fp=20</code>.\n+This value was chosen as during simulations it was the value that minimized the\n+bandwidth utilized by the expected number of blocks downloaded due to false\n+positives, and the bandwidth used to download the filters themselves. The code\n+along with a demo used for the parameter tuning can be found [here https://github.com/Roasbeef/bips/blob/83b83c78e189be898573e0bfe936dd0c9b99ecb9/gcs_light_client/gentestvectors.go]\n+\n+\n+=== Protocol Version Bump ===\n+\n+As this BIP defines new peer-to-peer behavior, we bump the protocol version by\n+one in order to distinguish the newly defined behavior. Full-nodes implementing\n+this BIP should advertise a protocol version of: <code>70016</code>.\n+\n+== New Wallet Capabilities Enabled ==\n+\n+The new light client mode enables wallet to maintain a very compact client-side\n+index of (possibly) the entire chain. Such an index provides a great degree of\n+utility for wallets, as they're now able to perform tasks such as private key\n+imports and full HD-seed imports without the need of a trusted third-party\n+server. Additionally, the compact client-side chain index also opens up the\n+door to smart contract applications which require agent action in response to\n+on-chain events. Examples of such applications include Lightning.\n+\n+== Backwards Compatability ==\n+\n+This light client protocl is NOT backwards compatible with BIP 37. Full nodes\n+MAY implement both protocols to serve both types of light clients.\n+\n+\n+== Implementation Notes ==\n+\n+This filter header chain SHOULD be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP SHOULD also fetch all the ''filter headers'' from\n+''each'' of their connected peers. With these headers, light clients SHOULD\n+efficiently detect nodes that advertise a conflicting filter chain history. To\n+do this, light clients MUST ensure that all nodes return the ``same` filter\n+header hash for a particular block header hash. \n+\n+Light clients MAY use the filter header chain to verify purported filter\n+authenticity when fetching the next set of headers from chain tip. Light\n+clients MAY use the following algorithm to more efficiently verify the\n+authenticity of filters (the naive version would fetch the entire filter from\n+each peer, this version saves bandwidth):\n+\n+<pre>\n+verify_from_tip(tip_block_hash: [32]byte):\n+    let filter_types = {supported_filter_types...}\n+    let connected_peers = {list_of_connected_full_nodes...}\n+\n+    for filter_type in filter_types:\n+\n+        let filter_headers = set()\n+        for peer in connected_peers:\n+            let filter_header = peer.fetch_filter_header(tip_block_hash)\n+            filter_headers.insert(filter_header)\n+\n+        if len(filter_headers) != 1:\n+            // Peers have conflicting filters. The light client should fetch\n+            // each unique filter from the set of peers AND fetch the block. The\n+            // light client can then verify which filter header is correct, and\n+            // BAN the offending peers.\n+\n+        // Otherwise, syncing continues as normal: fetch filter to see if it\n+        // matches any relevant items.\n+</pre>\n+\n+Light clients MAY persistently commit all filter headers to disk, as when\n+lazily fetching filters (due to a historical re-scan or chain analysis),\n+they're able to verify the authenticity of any fetched filters.\n+\n+Full-nodes MAY persistently compute and persist the filter header chain\n+on-disk, just as the regular filters.\n+\n+Zero-length filters are sent without an <code>N</code> value, allowing us to save\n+<code>4-bytes</code>. Clients are able to verify that a filter will be\n+<code>null</code> before requesting it (as it will just be the prior filter\n+header hashed with zero bytes). Clients can take this fact into account in\n+order to save a round trip for <code>null</code> headers.\n+\n+Light clients implementing this proposal SHOULD utilize the\n+<code>sendheaders</code> message. This allow quicker syncing from tip, as rather\n+than sending an <code>inv</code> to announce a new blocks, nodes will instead\n+directly send the <code>headers</code> message. With this, nodes can save a\n+round-trip and immediately request\n+<code>cfheaders</code> from each of their connected peers before ultimately\n+fetching the filter itself.\n+\n+Light client syncing MAY be run in reverse, meaning fetching the regular\n+Bitcoin and filter headers from the ''end'' of the chain and working backwards.\n+This allows a client to nearly ''instantly'' be useable if an application doesn't\n+require immediate access to historical filter data.\n+\n+If fetching blocks directly over the p2p network (rather than via a distinct\n+channel), light clients SHOULD fetch blocks from multiple peers in order to\n+mitigate transaction intersection analysis.\n+\n+Full nodes serving the light clients described in this BIP can be implemented\n+in a completely ''stateless'' manner. Implementations can also make a\n+space/time tradeoff by only holding filters on disk to some particular block\n+horizon. If a filter beyond this horizon is requested, then it can be\n+reconstructed in real time. This allows nodes to save on disk-space, as it's\n+likely the older filters would only be fetched for historical rescans. It is\n+also likely that the past few hundred filters will be fetched mostly frequently\n+by smaller devices (phones, laptops, etc) periodically coming back online after a\n+period of inactivity.\n+\n+It is possible to implementations of this BIP to serve ''other''\n+implementations to a degree. All headers (regular bitcoin, regular, extended)\n+can be served, and also any on-disk filters can also be served to other light\n+clients.\n+\n+Key import and rescan: with lazy <code>filters</code> fetching, having a start",
      "path": "gcs_light_client.mediawiki",
      "position": 874,
      "original_position": 874,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "What is \"lazy filters fetching\"?",
      "created_at": "2017-11-30T17:23:02Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154144526",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154144526"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 874,
      "original_line": 874,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154145945",
      "pull_request_review_id": 80257286,
      "id": 154145945,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE0NTk0NQ==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:",
      "path": "gcs_light_client.mediawiki",
      "position": 594,
      "original_position": 594,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Maybe I'm just not a Go person, but I find this pseudocode very hard to read... match filter(n) case Some case None where Some and None are never previously defined in the document? Without the comments it would be very hard to decipher...maybe something like if (filter(n) has elements): ?",
      "created_at": "2017-11-30T17:28:28Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154145945",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154145945"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 594,
      "original_line": 594,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154148392",
      "pull_request_review_id": 80257286,
      "id": 154148392,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE0ODM5Mg==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))",
      "path": "gcs_light_client.mediawiki",
      "position": 595,
      "original_position": 595,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Given this construction I see no reason to differentiate between the \"some data\" and \"no data\" case - double-sha-256(null string) is well-defined and you're concatenating hashes so it doesn't lead to any confusion.",
      "created_at": "2017-11-30T17:37:31Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154148392",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154148392"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 595,
      "original_line": 595,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154148812",
      "pull_request_review_id": 80257286,
      "id": 154148812,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE0ODgxMg==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single",
      "path": "gcs_light_client.mediawiki",
      "position": 597,
      "original_position": 597,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This is confusing as its only true for the Extended filter, not the Normal filter (as the Normal filter would contain at least the coinbase' txid and output script data pushes).",
      "created_at": "2017-11-30T17:39:08Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154148812",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154148812"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 597,
      "original_line": 597,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154149153",
      "pull_request_review_id": 80257286,
      "id": 154149153,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE0OTE1Mw==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))",
      "path": "gcs_light_client.mediawiki",
      "position": 589,
      "original_position": 589,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Heh, since you redid the section now this is even more confusing....genesis_block.prevblock reads to me like a reference to its actual previous block (which is undefined) not the previous block hash field in the header. Maybe swap back to zero_hash instead since its now clear that this is only for the genesis block or just put \"hash\" in there? Sorry for the back-and-forth.",
      "created_at": "2017-11-30T17:40:29Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154149153",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154149153"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 589,
      "original_line": 589,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154149478",
      "pull_request_review_id": 80257286,
      "id": 154149478,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE0OTQ3OA==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase",
      "path": "gcs_light_client.mediawiki",
      "position": 611,
      "original_position": 611,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Same comment as above, if I understand the construction correctly, good to note that this is only true for the Extended filter.",
      "created_at": "2017-11-30T17:42:00Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154149478",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154149478"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 611,
      "original_line": 611,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154150005",
      "pull_request_review_id": 80257286,
      "id": 154150005,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE1MDAwNQ==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes",
      "path": "gcs_light_client.mediawiki",
      "position": 633,
      "original_position": 633,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Why use block locators here? It seems like you're endeavoring to allow people to fetch the filters chain entirely separately from headers, but I dont know why anyone would ever want to do this - you still need the headers themselves to check things like PoW.",
      "created_at": "2017-11-30T17:44:03Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154150005",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154150005"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 633,
      "original_line": 633,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154151514",
      "pull_request_review_id": 80257286,
      "id": 154151514,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE1MTUxNA==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a",
      "path": "gcs_light_client.mediawiki",
      "position": 652,
      "original_position": 652,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This seems too restrictive (and conflicts with the note that locators \"have the same semantics as getheaders\" above. There are cases where getheaders messages are currently not responded to (hashStop set but locator empty, which has very different semantics as well as if a node thinks its in IBD so its headers response is going to be useless...note obviously things are easier to define if you drop the locator/stop thing in favor of a simpler startHash/blockCount request and restrict requests to hashes for blocks which were recently received in headers messages from the same node (something you're gonna get from essentially all your peers anyway).",
      "created_at": "2017-11-30T17:50:22Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154151514",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154151514"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 652,
      "original_line": 652,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154157805",
      "pull_request_review_id": 80257286,
      "id": 154157805,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE1NzgwNQ==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash",
      "path": "gcs_light_client.mediawiki",
      "position": 663,
      "original_position": 663,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This is racy - lets say you request some filter headers (eg from a peer just finishing re-syncing after restart), so the request ends with a block hash that you dont have the header for, but the network converges on a different block...now you have a filter chain ending in a block that you dont have a header for and possibly cant get, but needlessly so. Switching things to backwards download (eg making getcfheaders take a block hash and then a number of blocks *before* that block to return filter headers for) should make this much less of an issue. Or, ideally, just make it a getcfheader request instead of getcfheaders, simplifes a lot of things and you can still do a backwards walk with a binary search to find the conflicting header, something you'll want to do either way.",
      "created_at": "2017-11-30T18:16:16Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154157805",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154157805"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 663,
      "original_line": 663,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154157929",
      "pull_request_review_id": 80257286,
      "id": 154157929,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE1NzkyOQ==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.",
      "path": "gcs_light_client.mediawiki",
      "position": 670,
      "original_position": 670,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Need to specify somewhere that this needs to be the same value as the corresponding request.",
      "created_at": "2017-11-30T18:16:49Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154157929",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154157929"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 670,
      "original_line": 670,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154159011",
      "pull_request_review_id": 80257286,
      "id": 154159011,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE1OTAxMQ==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can SHOULD use these two messages to request a\n+compact filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message MUST be sent in response to a",
      "path": "gcs_light_client.mediawiki",
      "position": 708,
      "original_position": 708,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This is too restrictive. Need to specify what the restrictions are on the blockhash field (eg that it is something for which you've recently received a headers message from the peer for or so). Might also consdier clarifying that you should respond with the data for the same block and filter type  as the request :p.",
      "created_at": "2017-11-30T18:21:14Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154159011",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154159011"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 708,
      "original_line": 708,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154161739",
      "pull_request_review_id": 80257286,
      "id": 154161739,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE2MTczOQ==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can SHOULD use these two messages to request a\n+compact filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message MUST be sent in response to a\n+<code>getcfilter</code> message for a particular block hash.The\n+<code>cfilter</code> message is defined as follows:\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the filter is being returned.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter being returned.\n+|-\n+| Var-Int\n+| NumFilterBytes\n+| uint64\n+| A variable length integer encoding the number of bytes of the filter in the following field.\n+|-\n+| NumFilterBytes\n+| FilterBytes\n+| [NumFilterBytes]byte\n+| The raw compressed compact filter for this block.\n+|}\n+\n+The <code>BlockHash</code> field is included in both messages as this allows\n+easily matching requests against responses, as the responses aren't\n+self-identifying like block headers are (via own hash).\n+\n+The parameters <code>N</code> (the number of elements in the filter) and\n+<code>P</code> (<code>1 << false_positive_rate</code>) are required by the light\n+client in order to properly incrementally decode, query, and validate\n+(reconstruct from Bitcoin block) a compact filter. The parameter <code>N</code>\n+cannot be known ahead of time, therefore we define the serialization of a\n+compact filter of type <code>0x00</code> and <code>0x01</code> as:\n+<pre>\n+N || raw_filter_bytes",
      "path": "gcs_light_client.mediawiki",
      "position": 749,
      "original_position": 749,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Why not just put this in the message encoding? If a future new filter type no longer needs the N to be encoded, it can redefine the message encoding then.",
      "created_at": "2017-11-30T18:31:27Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154161739",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154161739"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 749,
      "original_line": 749,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154162515",
      "pull_request_review_id": 80257286,
      "id": 154162515,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE2MjUxNQ==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can SHOULD use these two messages to request a\n+compact filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message MUST be sent in response to a\n+<code>getcfilter</code> message for a particular block hash.The\n+<code>cfilter</code> message is defined as follows:\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the filter is being returned.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter being returned.\n+|-\n+| Var-Int\n+| NumFilterBytes\n+| uint64\n+| A variable length integer encoding the number of bytes of the filter in the following field.\n+|-\n+| NumFilterBytes\n+| FilterBytes\n+| [NumFilterBytes]byte\n+| The raw compressed compact filter for this block.\n+|}\n+\n+The <code>BlockHash</code> field is included in both messages as this allows\n+easily matching requests against responses, as the responses aren't\n+self-identifying like block headers are (via own hash).\n+\n+The parameters <code>N</code> (the number of elements in the filter) and\n+<code>P</code> (<code>1 << false_positive_rate</code>) are required by the light\n+client in order to properly incrementally decode, query, and validate\n+(reconstruct from Bitcoin block) a compact filter. The parameter <code>N</code>\n+cannot be known ahead of time, therefore we define the serialization of a\n+compact filter of type <code>0x00</code> and <code>0x01</code> as:\n+<pre>\n+N || raw_filter_bytes\n+</pre>\n+where <code>N</code> is serialized as a 32-bit big-endian integer.\n+\n+However, there exists a special case of a <code>null</code> filter. This this\n+case an empty byte slice is transmitted rather than consuming\n+<code>4-bytes</code> to encode the size of zero.\n+\n+However, as the parameter <code>P</code> ''MUST'' be globally agreed upon (for\n+a particular filter type), we define this value ''statically'' for filter\n+types: <code>0x00</code> and <code>0x01</code>. For the two aforementioned\n+filter types, the false positive rate MUST be: <code>20</code>, meaning the\n+parameter <code>P</code> is: <code>2^20</code>, meaning <code>fp=20</code>.\n+This value was chosen as during simulations it was the value that minimized the\n+bandwidth utilized by the expected number of blocks downloaded due to false\n+positives, and the bandwidth used to download the filters themselves. The code\n+along with a demo used for the parameter tuning can be found [here https://github.com/Roasbeef/bips/blob/83b83c78e189be898573e0bfe936dd0c9b99ecb9/gcs_light_client/gentestvectors.go]\n+\n+\n+=== Protocol Version Bump ===\n+\n+As this BIP defines new peer-to-peer behavior, we bump the protocol version by\n+one in order to distinguish the newly defined behavior. Full-nodes implementing\n+this BIP should advertise a protocol version of: <code>70016</code>.\n+\n+== New Wallet Capabilities Enabled ==\n+\n+The new light client mode enables wallet to maintain a very compact client-side\n+index of (possibly) the entire chain. Such an index provides a great degree of\n+utility for wallets, as they're now able to perform tasks such as private key\n+imports and full HD-seed imports without the need of a trusted third-party\n+server. Additionally, the compact client-side chain index also opens up the\n+door to smart contract applications which require agent action in response to\n+on-chain events. Examples of such applications include Lightning.\n+\n+== Backwards Compatability ==\n+\n+This light client protocl is NOT backwards compatible with BIP 37. Full nodes\n+MAY implement both protocols to serve both types of light clients.\n+\n+\n+== Implementation Notes ==\n+\n+This filter header chain SHOULD be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP SHOULD also fetch all the ''filter headers'' from\n+''each'' of their connected peers. With these headers, light clients SHOULD\n+efficiently detect nodes that advertise a conflicting filter chain history. To\n+do this, light clients MUST ensure that all nodes return the ``same` filter\n+header hash for a particular block header hash. \n+\n+Light clients MAY use the filter header chain to verify purported filter\n+authenticity when fetching the next set of headers from chain tip. Light\n+clients MAY use the following algorithm to more efficiently verify the\n+authenticity of filters (the naive version would fetch the entire filter from\n+each peer, this version saves bandwidth):\n+\n+<pre>\n+verify_from_tip(tip_block_hash: [32]byte):\n+    let filter_types = {supported_filter_types...}\n+    let connected_peers = {list_of_connected_full_nodes...}\n+\n+    for filter_type in filter_types:\n+\n+        let filter_headers = set()\n+        for peer in connected_peers:\n+            let filter_header = peer.fetch_filter_header(tip_block_hash)\n+            filter_headers.insert(filter_header)\n+\n+        if len(filter_headers) != 1:\n+            // Peers have conflicting filters. The light client should fetch\n+            // each unique filter from the set of peers AND fetch the block. The\n+            // light client can then verify which filter header is correct, and\n+            // BAN the offending peers.",
      "path": "gcs_light_client.mediawiki",
      "position": 823,
      "original_position": 823,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Something something binary search?",
      "created_at": "2017-11-30T18:33:58Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154162515",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154162515"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 823,
      "original_line": 823,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154162783",
      "pull_request_review_id": 80257286,
      "id": 154162783,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE2Mjc4Mw==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning",
      "path": "gcs_light_client.mediawiki",
      "position": 571,
      "original_position": 571,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I might suggest you strike this portion - you later say that clients should detect and ban nodes that dont have a normative filter for each block, so its a kind of consensus, even if not \"consensus critical\" per se.",
      "created_at": "2017-11-30T18:35:03Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154162783",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154162783"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 571,
      "original_line": 571,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154162999",
      "pull_request_review_id": 80257286,
      "id": 154162999,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE2Mjk5OQ==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can SHOULD use these two messages to request a\n+compact filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message MUST be sent in response to a\n+<code>getcfilter</code> message for a particular block hash.The\n+<code>cfilter</code> message is defined as follows:\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the filter is being returned.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter being returned.\n+|-\n+| Var-Int\n+| NumFilterBytes\n+| uint64\n+| A variable length integer encoding the number of bytes of the filter in the following field.\n+|-\n+| NumFilterBytes\n+| FilterBytes\n+| [NumFilterBytes]byte\n+| The raw compressed compact filter for this block.\n+|}\n+\n+The <code>BlockHash</code> field is included in both messages as this allows\n+easily matching requests against responses, as the responses aren't\n+self-identifying like block headers are (via own hash).\n+\n+The parameters <code>N</code> (the number of elements in the filter) and\n+<code>P</code> (<code>1 << false_positive_rate</code>) are required by the light\n+client in order to properly incrementally decode, query, and validate\n+(reconstruct from Bitcoin block) a compact filter. The parameter <code>N</code>\n+cannot be known ahead of time, therefore we define the serialization of a\n+compact filter of type <code>0x00</code> and <code>0x01</code> as:\n+<pre>\n+N || raw_filter_bytes\n+</pre>\n+where <code>N</code> is serialized as a 32-bit big-endian integer.\n+\n+However, there exists a special case of a <code>null</code> filter. This this\n+case an empty byte slice is transmitted rather than consuming\n+<code>4-bytes</code> to encode the size of zero.\n+\n+However, as the parameter <code>P</code> ''MUST'' be globally agreed upon (for\n+a particular filter type), we define this value ''statically'' for filter\n+types: <code>0x00</code> and <code>0x01</code>. For the two aforementioned\n+filter types, the false positive rate MUST be: <code>20</code>, meaning the\n+parameter <code>P</code> is: <code>2^20</code>, meaning <code>fp=20</code>.\n+This value was chosen as during simulations it was the value that minimized the\n+bandwidth utilized by the expected number of blocks downloaded due to false\n+positives, and the bandwidth used to download the filters themselves. The code\n+along with a demo used for the parameter tuning can be found [here https://github.com/Roasbeef/bips/blob/83b83c78e189be898573e0bfe936dd0c9b99ecb9/gcs_light_client/gentestvectors.go]\n+\n+\n+=== Protocol Version Bump ===\n+\n+As this BIP defines new peer-to-peer behavior, we bump the protocol version by\n+one in order to distinguish the newly defined behavior. Full-nodes implementing\n+this BIP should advertise a protocol version of: <code>70016</code>.\n+\n+== New Wallet Capabilities Enabled ==\n+\n+The new light client mode enables wallet to maintain a very compact client-side\n+index of (possibly) the entire chain. Such an index provides a great degree of\n+utility for wallets, as they're now able to perform tasks such as private key\n+imports and full HD-seed imports without the need of a trusted third-party\n+server. Additionally, the compact client-side chain index also opens up the\n+door to smart contract applications which require agent action in response to\n+on-chain events. Examples of such applications include Lightning.\n+\n+== Backwards Compatability ==\n+\n+This light client protocl is NOT backwards compatible with BIP 37. Full nodes\n+MAY implement both protocols to serve both types of light clients.\n+\n+\n+== Implementation Notes ==\n+\n+This filter header chain SHOULD be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP SHOULD also fetch all the ''filter headers'' from\n+''each'' of their connected peers. With these headers, light clients SHOULD\n+efficiently detect nodes that advertise a conflicting filter chain history. To\n+do this, light clients MUST ensure that all nodes return the ``same` filter\n+header hash for a particular block header hash. \n+\n+Light clients MAY use the filter header chain to verify purported filter\n+authenticity when fetching the next set of headers from chain tip. Light\n+clients MAY use the following algorithm to more efficiently verify the\n+authenticity of filters (the naive version would fetch the entire filter from\n+each peer, this version saves bandwidth):\n+\n+<pre>\n+verify_from_tip(tip_block_hash: [32]byte):\n+    let filter_types = {supported_filter_types...}\n+    let connected_peers = {list_of_connected_full_nodes...}\n+\n+    for filter_type in filter_types:\n+\n+        let filter_headers = set()\n+        for peer in connected_peers:\n+            let filter_header = peer.fetch_filter_header(tip_block_hash)\n+            filter_headers.insert(filter_header)\n+\n+        if len(filter_headers) != 1:\n+            // Peers have conflicting filters. The light client should fetch\n+            // each unique filter from the set of peers AND fetch the block. The\n+            // light client can then verify which filter header is correct, and\n+            // BAN the offending peers.\n+\n+        // Otherwise, syncing continues as normal: fetch filter to see if it\n+        // matches any relevant items.\n+</pre>\n+\n+Light clients MAY persistently commit all filter headers to disk, as when\n+lazily fetching filters (due to a historical re-scan or chain analysis),\n+they're able to verify the authenticity of any fetched filters.\n+\n+Full-nodes MAY persistently compute and persist the filter header chain\n+on-disk, just as the regular filters.\n+\n+Zero-length filters are sent without an <code>N</code> value, allowing us to save\n+<code>4-bytes</code>. Clients are able to verify that a filter will be\n+<code>null</code> before requesting it (as it will just be the prior filter\n+header hashed with zero bytes). Clients can take this fact into account in\n+order to save a round trip for <code>null</code> headers.\n+\n+Light clients implementing this proposal SHOULD utilize the",
      "path": "gcs_light_client.mediawiki",
      "position": 842,
      "original_position": 842,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "This seems very strong. Why SHOULD? Hell, why even MAY? It seems very unrelated.",
      "created_at": "2017-11-30T18:35:51Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154162999",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154162999"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 842,
      "original_line": 842,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154163289",
      "pull_request_review_id": 80257286,
      "id": 154163289,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE2MzI4OQ==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can SHOULD use these two messages to request a\n+compact filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message MUST be sent in response to a\n+<code>getcfilter</code> message for a particular block hash.The\n+<code>cfilter</code> message is defined as follows:\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the filter is being returned.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter being returned.\n+|-\n+| Var-Int\n+| NumFilterBytes\n+| uint64\n+| A variable length integer encoding the number of bytes of the filter in the following field.\n+|-\n+| NumFilterBytes\n+| FilterBytes\n+| [NumFilterBytes]byte\n+| The raw compressed compact filter for this block.\n+|}\n+\n+The <code>BlockHash</code> field is included in both messages as this allows\n+easily matching requests against responses, as the responses aren't\n+self-identifying like block headers are (via own hash).\n+\n+The parameters <code>N</code> (the number of elements in the filter) and\n+<code>P</code> (<code>1 << false_positive_rate</code>) are required by the light\n+client in order to properly incrementally decode, query, and validate\n+(reconstruct from Bitcoin block) a compact filter. The parameter <code>N</code>\n+cannot be known ahead of time, therefore we define the serialization of a\n+compact filter of type <code>0x00</code> and <code>0x01</code> as:\n+<pre>\n+N || raw_filter_bytes\n+</pre>\n+where <code>N</code> is serialized as a 32-bit big-endian integer.\n+\n+However, there exists a special case of a <code>null</code> filter. This this\n+case an empty byte slice is transmitted rather than consuming\n+<code>4-bytes</code> to encode the size of zero.",
      "path": "gcs_light_client.mediawiki",
      "position": 755,
      "original_position": 755,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Why the special case here? You're already sending many bytes anyway, dropping the extra 4 just seems...over optimization?",
      "created_at": "2017-11-30T18:36:54Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154163289",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154163289"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 755,
      "original_line": 755,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154163549",
      "pull_request_review_id": 80257286,
      "id": 154163549,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE2MzU0OQ==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can SHOULD use these two messages to request a\n+compact filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message MUST be sent in response to a\n+<code>getcfilter</code> message for a particular block hash.The\n+<code>cfilter</code> message is defined as follows:\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the filter is being returned.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter being returned.\n+|-\n+| Var-Int\n+| NumFilterBytes\n+| uint64\n+| A variable length integer encoding the number of bytes of the filter in the following field.\n+|-\n+| NumFilterBytes\n+| FilterBytes\n+| [NumFilterBytes]byte\n+| The raw compressed compact filter for this block.\n+|}\n+\n+The <code>BlockHash</code> field is included in both messages as this allows\n+easily matching requests against responses, as the responses aren't\n+self-identifying like block headers are (via own hash).\n+\n+The parameters <code>N</code> (the number of elements in the filter) and\n+<code>P</code> (<code>1 << false_positive_rate</code>) are required by the light\n+client in order to properly incrementally decode, query, and validate\n+(reconstruct from Bitcoin block) a compact filter. The parameter <code>N</code>\n+cannot be known ahead of time, therefore we define the serialization of a\n+compact filter of type <code>0x00</code> and <code>0x01</code> as:\n+<pre>\n+N || raw_filter_bytes\n+</pre>\n+where <code>N</code> is serialized as a 32-bit big-endian integer.\n+\n+However, there exists a special case of a <code>null</code> filter. This this\n+case an empty byte slice is transmitted rather than consuming\n+<code>4-bytes</code> to encode the size of zero.\n+\n+However, as the parameter <code>P</code> ''MUST'' be globally agreed upon (for\n+a particular filter type), we define this value ''statically'' for filter\n+types: <code>0x00</code> and <code>0x01</code>. For the two aforementioned\n+filter types, the false positive rate MUST be: <code>20</code>, meaning the\n+parameter <code>P</code> is: <code>2^20</code>, meaning <code>fp=20</code>.\n+This value was chosen as during simulations it was the value that minimized the\n+bandwidth utilized by the expected number of blocks downloaded due to false\n+positives, and the bandwidth used to download the filters themselves. The code\n+along with a demo used for the parameter tuning can be found [here https://github.com/Roasbeef/bips/blob/83b83c78e189be898573e0bfe936dd0c9b99ecb9/gcs_light_client/gentestvectors.go]\n+\n+\n+=== Protocol Version Bump ===\n+\n+As this BIP defines new peer-to-peer behavior, we bump the protocol version by\n+one in order to distinguish the newly defined behavior. Full-nodes implementing\n+this BIP should advertise a protocol version of: <code>70016</code>.\n+\n+== New Wallet Capabilities Enabled ==\n+\n+The new light client mode enables wallet to maintain a very compact client-side\n+index of (possibly) the entire chain. Such an index provides a great degree of\n+utility for wallets, as they're now able to perform tasks such as private key\n+imports and full HD-seed imports without the need of a trusted third-party\n+server. Additionally, the compact client-side chain index also opens up the\n+door to smart contract applications which require agent action in response to\n+on-chain events. Examples of such applications include Lightning.\n+\n+== Backwards Compatability ==\n+\n+This light client protocl is NOT backwards compatible with BIP 37. Full nodes\n+MAY implement both protocols to serve both types of light clients.\n+\n+\n+== Implementation Notes ==\n+\n+This filter header chain SHOULD be utilized by light clients to gain a greater",
      "path": "gcs_light_client.mediawiki",
      "position": 792,
      "original_position": 792,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "Maybe s/by light clients/in place of BIP 37/",
      "created_at": "2017-11-30T18:37:51Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154163549",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154163549"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 792,
      "original_line": 792,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154163993",
      "pull_request_review_id": 80257286,
      "id": 154163993,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE2Mzk5Mw==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can SHOULD use these two messages to request a\n+compact filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message MUST be sent in response to a\n+<code>getcfilter</code> message for a particular block hash.The\n+<code>cfilter</code> message is defined as follows:\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the filter is being returned.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter being returned.\n+|-\n+| Var-Int\n+| NumFilterBytes\n+| uint64\n+| A variable length integer encoding the number of bytes of the filter in the following field.\n+|-\n+| NumFilterBytes\n+| FilterBytes\n+| [NumFilterBytes]byte\n+| The raw compressed compact filter for this block.\n+|}\n+\n+The <code>BlockHash</code> field is included in both messages as this allows\n+easily matching requests against responses, as the responses aren't\n+self-identifying like block headers are (via own hash).\n+\n+The parameters <code>N</code> (the number of elements in the filter) and\n+<code>P</code> (<code>1 << false_positive_rate</code>) are required by the light\n+client in order to properly incrementally decode, query, and validate\n+(reconstruct from Bitcoin block) a compact filter. The parameter <code>N</code>\n+cannot be known ahead of time, therefore we define the serialization of a\n+compact filter of type <code>0x00</code> and <code>0x01</code> as:\n+<pre>\n+N || raw_filter_bytes\n+</pre>\n+where <code>N</code> is serialized as a 32-bit big-endian integer.\n+\n+However, there exists a special case of a <code>null</code> filter. This this\n+case an empty byte slice is transmitted rather than consuming\n+<code>4-bytes</code> to encode the size of zero.\n+\n+However, as the parameter <code>P</code> ''MUST'' be globally agreed upon (for\n+a particular filter type), we define this value ''statically'' for filter\n+types: <code>0x00</code> and <code>0x01</code>. For the two aforementioned\n+filter types, the false positive rate MUST be: <code>20</code>, meaning the\n+parameter <code>P</code> is: <code>2^20</code>, meaning <code>fp=20</code>.\n+This value was chosen as during simulations it was the value that minimized the\n+bandwidth utilized by the expected number of blocks downloaded due to false\n+positives, and the bandwidth used to download the filters themselves. The code\n+along with a demo used for the parameter tuning can be found [here https://github.com/Roasbeef/bips/blob/83b83c78e189be898573e0bfe936dd0c9b99ecb9/gcs_light_client/gentestvectors.go]\n+\n+\n+=== Protocol Version Bump ===\n+\n+As this BIP defines new peer-to-peer behavior, we bump the protocol version by\n+one in order to distinguish the newly defined behavior. Full-nodes implementing\n+this BIP should advertise a protocol version of: <code>70016</code>.\n+\n+== New Wallet Capabilities Enabled ==\n+\n+The new light client mode enables wallet to maintain a very compact client-side\n+index of (possibly) the entire chain. Such an index provides a great degree of\n+utility for wallets, as they're now able to perform tasks such as private key\n+imports and full HD-seed imports without the need of a trusted third-party\n+server. Additionally, the compact client-side chain index also opens up the\n+door to smart contract applications which require agent action in response to\n+on-chain events. Examples of such applications include Lightning.\n+\n+== Backwards Compatability ==\n+\n+This light client protocl is NOT backwards compatible with BIP 37. Full nodes\n+MAY implement both protocols to serve both types of light clients.\n+\n+\n+== Implementation Notes ==\n+\n+This filter header chain SHOULD be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP SHOULD also fetch all the ''filter headers'' from\n+''each'' of their connected peers. With these headers, light clients SHOULD",
      "path": "gcs_light_client.mediawiki",
      "position": 796,
      "original_position": 796,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "TheBlueMatt",
        "id": 649246,
        "node_id": "MDQ6VXNlcjY0OTI0Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/649246?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/TheBlueMatt",
        "html_url": "https://github.com/TheBlueMatt",
        "followers_url": "https://api.github.com/users/TheBlueMatt/followers",
        "following_url": "https://api.github.com/users/TheBlueMatt/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/TheBlueMatt/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/TheBlueMatt/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/TheBlueMatt/subscriptions",
        "organizations_url": "https://api.github.com/users/TheBlueMatt/orgs",
        "repos_url": "https://api.github.com/users/TheBlueMatt/repos",
        "events_url": "https://api.github.com/users/TheBlueMatt/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/TheBlueMatt/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I'd say they SHOULD only fetch the top header after doing a headers sync from all of their peers except one, from which they build the whole header tree.",
      "created_at": "2017-11-30T18:39:38Z",
      "updated_at": "2017-11-30T18:39:46Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154163993",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154163993"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 796,
      "original_line": 796,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154168822",
      "pull_request_review_id": 80291954,
      "id": 154168822,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDE2ODgyMg==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: ",
      "path": "gcs_light_client.mediawiki",
      "position": 409,
      "original_position": 409,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": 154142490,
      "user": {
        "login": "luke-jr",
        "id": 1095675,
        "node_id": "MDQ6VXNlcjEwOTU2NzU=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1095675?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/luke-jr",
        "html_url": "https://github.com/luke-jr",
        "followers_url": "https://api.github.com/users/luke-jr/followers",
        "following_url": "https://api.github.com/users/luke-jr/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/luke-jr/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/luke-jr/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/luke-jr/subscriptions",
        "organizations_url": "https://api.github.com/users/luke-jr/orgs",
        "repos_url": "https://api.github.com/users/luke-jr/repos",
        "events_url": "https://api.github.com/users/luke-jr/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/luke-jr/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "I agree. It would be ideal if pruned nodes could set this and still be usable by light wallets (I might even suggest that clients SHOULD support using pruned nodes... many users want pruned nodes, and their own node should be the expected server)",
      "created_at": "2017-11-30T18:57:28Z",
      "updated_at": "2017-11-30T18:57:28Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154168822",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154168822"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 409,
      "original_line": 409,
      "side": "RIGHT"
    },
    {
      "url": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154235642",
      "pull_request_review_id": 80369799,
      "id": 154235642,
      "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDIzNTY0Mg==",
      "diff_hunk": "@@ -0,0 +1,1056 @@\n+<pre>\n+BIP: ???\n+Layer: Peer Services\n+Title: Compact Client Side Filtering for Light Clients\n+Author: Olaoluwa Osuntokun <laolu32@gmail.com>\n+        Alex Akselrod <alex@akselrod.org>\n+Comments: ???\n+Comments-URI: ???\n+Type: Standards Track\n+Created: 05-24-2017\n+License: CC0-1.0\n+</pre>\n+\n+== Abstract ==\n+\n+This BIP describes a new light client node type for Bitcoin as well as the\n+modifications to current full-nodes required to support this new type of light\n+client. The light client mode described in this BIP is meant to supersede BIP\n+37 as it provides a greater degree of privacy, utility, and also reduces the\n+resources required for full-nodes to service this new light client mode\n+compared to BIP\n+37<ref>https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki</ref>.\n+The light client mode described in this BIP can be seen as a \"reversal\" of BIP\n+37<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-May/012636.html</ref>:\n+rather than the light clients sending filters to full-nodes, full-nodes send\n+filters to light clients. Unlike BIP 37, we don't utilize bloom filters.\n+Instead, we utilize a compact filter (more efficient than bloom filters) which\n+leverages Golomb-Rice coding for compression.  Additionally, blocks are\n+downloaded as a whole (from any source), rather than directly from peers as\n+fragments with merkle-branches proving their authenticity.\n+\n+== Motivation ==\n+\n+Light clients in Bitcoin provide applications with a less resource intensive\n+mechanism of validating the work of the most difficult chain and identifying\n+entries in the blockchain's log which are relevant to said application. In order\n+to accomplish the first, light clients download and verify the connectivity and\n+work of ''only'' the block headers of the chain. Block headers are a constant\n+80-bytes, resulting in minimal bandwidth even for very long chains. In order to\n+efficiently accomplish the second task (ascertaining relevant chain data) light\n+clients require a mechanism to learn of relevant data in blocks.\n+\n+BIP 37 is currently the most widely used light client execution mode within\n+Bitcoin. In BIP 37, rather than fetching and fully validating all blocks in the\n+chain, the light client instead verifies all headers and sends bloom filters\n+containing relevant data to full-nodes. These full-nodes then service the light\n+client by querying data within a block against the loaded bloom filter, if a\n+transaction matches the filter, a merkle-branch for the matching transaction is\n+sent and distinctly the transaction itself is sent.\n+\n+However, BIP 37 has several downsides. Bloom filtering as widely implemented\n+provides virtually ''zero'' privacy to wallets or other applications using this\n+mechanism\n+<ref>https://eprint.iacr.org/2014/763.pdf</ref><ref>https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/</ref>.\n+Additionally, applications are forced to carefully manage their false positive\n+rates in order to not completely give away their set of interested items.\n+Additionally, full-nodes can nearly undetectably lie by omission, causing a\n+denial of service which can lead to undesirable failure modes in applications\n+whose safety critically relies on responding to certain on-chain events. When\n+faithfully servicing BIP 37 light clients, full-nodes may incur significant I/O\n+and CPU resource usage due to maliciously crafted bloom filters, creating a\n+denial-of-service vector.\n+\n+The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n+\"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n+interpreted as described in RFC 2119.\n+\n+== Design Rationale ==\n+\n+In order to address the drawbacks of BIP 37 raised above, in this document we\n+propose an alternative chain filtering mechanism for light clients. Our\n+proposal sports a greater degree of privacy than BIP 37 as filtering is now\n+done on the client side. Clients download a deterministically generated filter\n+for a block and query it locally. If relevant items are found in the filter\n+then the ''entire'' block will be fetched. The decoupling of filter querying from\n+active communication with full-nodes enables light clients to fetch blocks from\n+''any'' source. Extremely privacy conscious light clients may opt to anonymously\n+fetch blocks using cryptographic techniques such a Private Information\n+Retrieval <ref>https://en.wikipedia.org/wiki/Private_information_retrieval</ref>.\n+\n+In order to reduce the size of the filter, we use a data structure capable of\n+probabilistic set membership. We elide the selection of the bloom filter data\n+structure in favor of utilising Golomb-Rice coding which allows us to generate\n+filters more compact than bloom filters which approach the theoretical minimum\n+size for probabilistic data structure.\n+\n+Light clients operating using the method described in this document are able to\n+verify the authenticity of filters received, thereby eliminating the ability\n+for full-nodes to lie by omission. Such client side filtering also improves the\n+utility of light clients for generic applications beyond simple wallets and\n+usage of basic public key templates. \n+\n+Finally, full-nodes only need to construct filters ''once'' as they're\n+deterministically generated for each block. Once the index is built, no further\n+active processing is required to serve light clients. Servicing light clients\n+simply entails reading pre-computed filters and headers from disk and sending\n+them over the network.\n+\n+\n+== Preliminaries ==\n+\n+Before we specify the details of our proposal, we'll first go over a few\n+preliminaries which will aid in the understanding our proposal.\n+\n+By <code>[]byte</code> we refer to a slice (or vector) of bytes. This value is\n+typically expressed in C-like languages as an array of uint_8's.\n+\n+By <code>Var-Int</code> we refer to the variable length integer encoding used\n+widely within the Bitcoin p2p protocol as an efficient way to encode the number\n+of items in a repeated series of items. The p2p message extensions in this\n+proposal will utilize this variable-length integer encoding in an identical\n+manner to the existing Bitcoin p2p messages.\n+\n+By <code>siphash(k, n)</code> we refer to an invocation of the <code>SipHash</code>\n+pseudo-random function with <code>k</code> as the 128-bit key, and <code>n</code>\n+as the input to the PRF. We instantiate with the recommended parameters of\n+<code>c = 2</code> and <code>d = 4</code>.\n+\n+We define the concept of an abstract bit stream instantiated by the function:\n+<code>new_bit_stream</code> The <code>bit_stream</code> has two functions that\n+operate on it, <code>unary_encode(stream, n)</code> and\n+<code>write_bits_big_endian(stream, n, k)</code> where <code>unary_encode(steam,\n+n)</code> emits n (an integer) to the stream in unary, and\n+<code>write_bits_big_endian(stream, n, k)</code> emits the lower <code>k</code>\n+bits of n to the stream using a big-endian binary encoding. For our unary\n+encoding, we encode a series of 1's followed by a terminating 0.\n+\n+Whenever we reference sorting, we refer to an ascending sorted order. The items\n+in a sorted set should increase from smallest index to largest index.\n+\n+We use a form of pseudo-code throughout the specification. In some areas we use\n+pattern-matching to specify the details of an algorithm:\n+\n+* <code>match(ITEM)</code> denotes a clause which pattern matches on <code>ITEM</code> (similar to a switch statement in imperative languages).\n+* <code>Some</code> denotes a non-empty <code>ITEM</code>, equivalent to a non-nil pointer or value\n+* <code>None</code> denotes an empty <code>ITEM</code>, equivalent to a nil pointer or value\n+\n+== Specification == \n+\n+=== Compact Chain Filters === \n+\n+In this BIP, we propose that light clients be provided with compact filters\n+which succinctly encode the contents of blocks. Instead of bloom filters, we\n+instead employ a data structure which is a compressed version of the hashed\n+values of the contents of blocks. Throughout the document, we refer to this\n+data structure as a Golomb Coded Set (GCS). At a high level the set contains a\n+list of sorted fixed size values. These values are then compressed using a type\n+of run length encoding. In order to query the set, it must be decompressed. \n+\n+We will now define simple functions for encoding and decoding integers using\n+Golomb-Rice <ref>https://en.wikipedia.org/wiki/Golomb_coding#Rice_coding</ref>\n+coding. These functions will be used in the next section as a primitive in the\n+construction of our compact filters.\n+\n+<pre>\n+golomb_encode(stream, n, k):\n+    let q = n >> k\n+    unary_encode(stream, q)\n+    write_bits_big_endian(stream, n, k)\n+</pre>\n+\n+<pre>\n+golomb_decode(stream, k) -> int:\n+    let c = stream.read_bit()\n+\n+    let n = 0\n+    while c == 0:\n+        n++\n+        c = stream.read_bit()\n+\n+    let r = b.read_bits_big_endian(k)\n+\n+    where read_bits_big_endian(k) decodes a fixed-length big-endian integer of\n+        k-bits \n+\n+    c*m + r\n+</pre>\n+\n+With the two functions above, we're able to efficiently compress a single\n+integer using Golomb-Rice coding. In the next section, we'll put everything\n+together and use the primitives described above to construct our compact sets.\n+\n+=== Golomb-Rice Coded Sets ===\n+\n+Rather than insert items directly into our set, we instead first run the items\n+through a PRF. This creates a set of uniformly distributed values. If we then\n+sort each of these values, the ''delta'' between each of the values closely\n+resembles a ''Geometric Distribution''. We'll again leverage this fact to use\n+Golomb-Rice coding to compresses our set by only encoding the ''delta'' between\n+two successive elements in the set.\n+\n+Golomb-Rice coded sets take two parameters: \n+* <code>N</code> the number of items to be inserted into the set\n+* <code>P</code> a value which is computed as <code>1/fp</code> where <code>fp</code> is the desired false positive rate. \n+\n+Given these two parameters, we can now construct our set.\n+\n+==== Set Construction ====\n+\n+Set construction takes three parameters: <code>N</code>, <code>P</code> and\n+<code>L</code>\n+* where <code>L</code> is a list of the ''raw'' items we wish to insert into the set\n+* the type of <code>L</code> is assumed to be of <code>[]byte</code>\n+\n+NOTE: <code>P</code> ''must'' be a power of two as we target the specialized case of\n+Golomb coding: Golomb-Rice coding.\n+\n+Using <code>N</code> and <code>P</code> we compute <code>F = N * P</code>.\n+<code>F</code> constricts the range of the hashed values accordingly in order\n+to achieve our desired false positive rate.\n+\n+In addition, to help optimize the algorithm, we use a fast range\n+algorithm<ref>https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/</ref>,\n+multiplying the hashed value by F and taking only the top 64 bits. This fairly\n+distributes the values over F without expensive division operations. In our\n+domain, the operation will use 64-bit integers. As a result, one may need to\n+manually compute the upper 64-bits of a 64-bits integers multiplication. This\n+can be done with fewer cycles on CPUs that have 128-bit registers. We use\n+64-bits, as this is the outputs size of siphash(2, 4).\n+\n+The following routine computes the ''uncompressed'' set given the parameters\n+above:\n+<pre>\n+hashed_set_construct(N, P, raw_items, k): -> []uint64:\n+    let F = N * P\n+\n+    let set_items = []\n+    for item in raw_items:\n+        let set_value = (siphash(k, item) * F) >> 64\n+        set_items.append(set_value)\n+\n+    // Sorts in ascending order.\n+    set_items.sort()\n+\n+    set_items\n+</pre>\n+\n+Using the routine above, we can transform our set of (possibly heterogeneous\n+items) in to a list of uniformly distributed values. As a final step, these\n+values are then sorted. When sorting then items MUST be ordered in ascending\n+order.\n+\n+==== Set Compression ====\n+\n+Once the set of hashed items has been constructed (and sorted in ascending\n+order) we then use Golomb-Rice coding to ''compress'' the set by encoding the\n+''delta'' value between each successive element within the set. As these values\n+are uniformly distributed, the deltas between these values will be\n+Geometrically Distributed, meaning that Golomb-Rice coding will be optimal for\n+this use-case <ref>http://urchin.earth.li/~twic/Golombs_Original_Paper/</ref>.\n+\n+The following routine describes the compression process:\n+<pre>\n+gcs_compress(sorted_set, fp) -> []byte:\n+    let stream = new_bit_stream()\n+\n+    // P is equivalent to m, the size of a golomb code-word.\n+    let P = 1 << fp\n+\n+    let last_value = 0\n+    for value in sorted_set:\n+        // Compute the difference between this value and the last value modulo\n+        // P.\n+        let remainder = (value - last_value) & (P - 1)\n+\n+        // Compute the difference between this value and the last one, divided\n+        // by P. This is our quotient.\n+        let quotient = (value - last_value - remainder) >> fp\n+\n+        // Write out the quotient value in unary into the bit stream.\n+        unary_encode(stream, quotient)\n+\n+        // Finally, write the remainder into the bit stream using fp bits.\n+        write_bits_big_endian(stream, remainder, fp)\n+\n+        // Track this value so we can use it compute the diff between this\n+        // value and the last.\n+        last_value = value\n+\n+    stream.bytes()\n+</pre>\n+\n+The routine above computes a ''compressed'' set using Golomb-Rice coding to\n+encode the ''delta'' between elements within the set. Unlike a bloom-filter,\n+this data-structure cannot be queried in its current form. Instead, one MUST\n+first perform the reverse computation to ''decompress'' the items in the set,\n+revealing the true values which can be queried against.\n+\n+==== Set Querying/Decompression ====\n+\n+Given a compressed Golomb-Rice coded set, one MUST first ''decompress'' the set\n+itself in order to query items which have been included within the set.\n+Decompression of a set follows the reverse procedure of encoding. To decode an\n+element, we'll decode the encoded quotient and remainder of encoded delta.\n+With the full delta re-constructed, we then ''add'' this value to the prior\n+value in order to reconstruct the full value. Following this procedure we can\n+incrementally decompress the set lazily without decompressing the entire\n+filter.\n+\n+===== Querying for a Single Item =====\n+\n+The following routing describes how one queries a compressed set for a ''single''\n+item:\n+<pre>\n+gcs_match(key: [16]byte, compressed_set: []byte, target: []byte, fp, N: int) -> bool:\n+    // First we'll map the item into the domain of our encoding.\n+    let item = (siphash(key, target) * (N * (1 << fp))) >> 64\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // We initialize the initial accumulator to a value of zero.\n+    let last_value = 0\n+\n+    // As the values in the set are sorted once the decoded values exceeds the\n+    // value we wish to query for, we can terminate our search early.\n+    for last_value < item:\n+        // Read the delta between this value and the next value which has been\n+        // encoded using Golomb-Rice codes.\n+        let decoded_value = golomb_decode(stream, fp)\n+\n+        // With the delta computed, we can now reconstruct the original value.\n+        let set_item = last_value + decoded_value\n+\n+        // If the values match up, then the target item _may_ be in the set, so\n+        // we return true.\n+        if set_item == item:\n+            true\n+\n+        last_value = set_item\n+\n+    // If we reach this point, then the item isn't in the set.\n+    false\n+</pre>\n+\n+===== Querying Against a Set of Items =====\n+\n+For most applications, the common case will be attempting to match a ''list'' of\n+items to the filter. In this case, we can perform a \"zip\" search against two\n+sorted lists: the step-by-step decompressed values of the set, and the list of\n+items we'd like to query.\n+\n+The following routine will evaluate to ''true'' if ''any'' of the items in a\n+target set are ''maybe'' within the original set of items (pre encoding):\n+<pre>\n+gcs_match_any(key: [16]byte, compressed_set: []byte, targets [][]byte, \n+              fp, N: int) -> bool:\n+\n+    stream = new_bit_stream(compressed_set)\n+\n+    // Once again, we'll map our set of target values into the domain our\n+    // encoding, sorting as a last step so we can zip through the values.\n+    let items = []\n+    for t in target:\n+        let item = (siphash(key, t) * (N * (1 << fp))) >> 64\n+        items.append(item)\n+    items.sort()\n+\n+    // Set up a set of accumulator values that we'll use to zip down the two\n+    // filters.\n+    let last_set_val, last_target_val = 0, 0 \n+    last_target_val = items[0]\n+    let = 1\n+\n+    // We'll keep running until one of the values matches each other. If this\n+    // happens, then we have a match!\n+    while last_set_val != last_target_val:\n+        // Perform a pattern match to decide which filter we'll need to\n+        // advance.\n+        match:\n+            case last_set_val > last_target_val:\n+                // If we still have items let, advance the pointer by one.\n+                if i < len(items):\n+                    last_target_val = items[i]\n+                    i++\n+\n+                // Otherwise, we've ran our items in our target set, which\n+                // means nothing matched.\n+                false\n+\n+            case last_target_val > last_set_val:\n+                // In this case, we'll advance the filter we're querying\n+                // against. This entails decompressing the next element in the\n+                // set.\n+                let decoded_value = golomb_decode(stream, fp)\n+\n+                // Accumulate the decoded delta value to the current value in\n+                // order to retrieve the current set item.\n+                last_set_val += decoded_value\n+\n+    // If we reach this point, the two items in the set matched!\n+    true\n+</pre>\n+\n+\n+=== Peer to Peer Network Extensions ===\n+\n+With the procedures to construct, compress, and query the sets explained, we'll\n+now turn to the modifications to Bitcoin's p2p protocol required to support\n+this new operating mode.\n+\n+\n+==== Peer to Peer Service Bit ====\n+\n+To start, we reserve a currently unutilized service bit. This is required as\n+light clients SHOULD ''preferentially'' peer to full-nodes that support the\n+features outlined in this BIP.\n+\n+The 6th service bit will now be dedicated to signaling support for the\n+features described within this BIP: \n+* <code>SFNodeCF = 1 << 6</code>\n+\n+==== Filter Types ====\n+\n+As this framework for client-side chain filtering is meant to be generic, in\n+this document we define two ''filter types''. A filter type denotes both the\n+construction/querying for a filter as well as the contents of the filter.\n+\n+At the time of writing of this BIP, two filter types are defined:\n+* Normal (<code>0x00</code>)\n+* Extended (<code>0x01</code>)\n+\n+A <code>Normal</code> filter is intended to contain all the items that a light client\n+needs to sync a basic Bitcoin wallet. In order to facilitate this use-case, for\n+each transaction, normal filters contain:\n+* The outpoints of each input within a transaction.\n+* The data-pushes contained within the public key script of each output within the transaction.\n+* The <code>txid</code> of the transaction itself.\n+\n+An <code>Extended</code> filter contains extra data that is meant to facilitate the\n+adoption of more advanced smart contracting applications by this BIP. For each\n+transaction found in a block, an <code>Extended</code> filter contains:\n+* Each item within the witness stack of an input (if the input has a witness).\n+* Each data push of the signature script of an input.\n+\n+Notably, this construction does not currently interpret P2SH scripts or witness\n+scripts to extract data pushes from them; however, future filter types may be\n+designed to do so.\n+\n+==== Filter Construction ====\n+\n+In order to ensure that filters are deterministically generated, we will use\n+the ''first'' <code>16-bytes</code> of the <code>block hash</code> of a Bitcoin\n+block as the key to our <code>siphash</code> function. Full-nodes that support\n+this BIP SHOULD treat the set of filters as an additional index of the\n+blockchain. Once a new block arrives, both filter types SHOULD be constructed,\n+and stored on disk. Full nodes MAY opt to dynamically construct the filters at\n+runtime, trading off space for additional computation. Full-nodes that update\n+to support this BIP once already synced, SHOULD upon start-up, re-index the\n+chain, constructing filters for each block from genesis to current chain tip.\n+\n+When indexing input and output scripts, we only index the push datas in the\n+script. The function <code>extract_push_datas</code> returns a vector of byte\n+slices that contain any pushed data found within the script. Pushed datas are\n+the byte slices following: <code>OP_PUSHDATA1</code>,\n+<code>OP_PUSHDATA2</code>, <code>OP_PUSHDATA4</code>, and the opcodes numbered\n+<code>1</code> to <code>75</code>.  The set of returned values includes\n+<code>OP_O</code>, but excludes <code>OP_1</code> - <code>OP_16</code>.\n+<code>OP_O</code> MUST be emitted as an empty byte slice.  For the complete set\n+of opcodes defined in Script, we refer the reader to\n+<ref>https://en.bitcoin.it/wiki/Script</ref>.\n+\n+Given a Bitcoin block, a full-node MUST construct a <code>Normal</code> compact\n+filter as follows:\n+\n+<pre>\n+construct_normal_gcs_filter(block, fp) -> []byte:\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        let txid = tx.hash()\n+        raw_items.append(txid)\n+\n+        for output in tx.outputs:\n+            let output_bytes = extract_push_datas(output.script)\n+            for output_byte in output_bytes:\n+                raw_items.append(output_byte)\n+\n+        if tx.is_coinbase():\n+            continue\n+\n+        for input in tx.inputs:\n+            // Inputs serialized as they are on the wire in transactions.\n+            // Input index serialized in little-endian.\n+            let input_bytes = input.hash || input.index\n+            raw_items.append(input_bytes)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+Given a Bitcoin block, a full-node MUST construct an <code>Extended</code>\n+compact filter as follows:\n+<pre>\n+construct_extended_gcs_filter(block, fp) -> []byte:\n+\n+    let siphash_key = block.hash()[:16]\n+\n+    let P = 1 << fp\n+\n+    let raw_items = []\n+    for tx in block.transactions:\n+        if tx.is_coinbase():\n+           continue\n+\n+        for input in tx.inputs:\n+            for wit_elem in input.witness:\n+                raw_items.append(wit_elem)\n+\n+            let sig_script_pushes = extract_push_datas(input.sig_script)\n+            for push in sig_script_pushes:\n+                raw_items.append(push)\n+\n+    let N = len(raw_items)\n+    let F = N * P\n+\n+    let hashed_items = []\n+    for raw_item in raw_items:\n+        let hashed_item = (siphash_key(siphash_key, raw_item) * F) >> 64\n+        hashed_items.append(hashed_item)\n+\n+    // Sorted in ascending order.\n+    hashed_items.sort()\n+\n+    gcs_compress(hashed_items, fp)\n+</pre>\n+\n+==== Filter Capability Querying ====\n+\n+As it's feasible that in the future, this document is extended to encompass\n+additional filter encoding algorithms or filter contents, we define a new p2p\n+message that allows light clients to ascertain which filters a node supports.\n+\n+The <code>getcftypes</code> message is an ''empty message'' whose command string is:\n+<code>getcftypes</code>\n+\n+A full-node that receives a <code>getcftypes</code> message MUST respond with a\n+<code>cftypes</code> message which is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumFilters\n+| uint64\n+| The number of supported filters.\n+|-\n+| NumFilters\n+| SupportedFilters\n+| [NumFilterBytes]byte\n+| A byte slice with each byte denoting a supported filter type\n+|}\n+\n+\n+==== Compact Filter Header Chain ====\n+\n+As the filters described in this BIP ''are not'' consensus critical, meaning\n+each filter is ``not`` validated by full-nodes and committed into blocks by\n+miners, we require an alternative (albeit less-binding) method to allow light\n+clients to ''identify'' and ''reject'' invalid filters. The purely p2p solution\n+to this problem is to obtain a deterministic hash-chain of ''each'' filter.\n+This hash chain or \"filter header chain\" is similar to the regular Bitcoin\n+headers in that it allows a light client to verify the ''authenticity'' of a\n+received\n+filter.\n+\n+The filter header chain for a particular filter type is described by the\n+following recurrence:\n+<pre>\n+filter_header(n: uint) -> [32]byte = \n+   // The zero hash is 32 bytes of 0's.\n+   let zero_hash [32]byte = {0*32}\n+\n+   if n == 0:\n+       double-sha-256(genesis_block.prevblock || filter(0))\n+\n+   match filter(n): \n+      // If the filter isn't empty, then we hash the filter itself into the\n+      // header chain.\n+      case Some:\n+          double-sha-256(filter_header(n-1) || double-sha-256(filter(n)))\n+\n+      // Otherwise, if the filter is empty (created from a block with a single\n+      // coinbase transaction whose output script contains no push datas), then\n+      // we'll hash the zero_hash.\n+      case None:\n+          double-sha-256(filter_header(n-1) || double-sha-256(zero_hash))\n+\n+   where filter(n) is the filter for block height n\n+</pre>\n+\n+The filter header for the genesis block uses the hash stored in the prevblock\n+field of the genesis block header itself, as there's no prior filter header\n+(by definition).\n+\n+Due to the nature of filter construction, it's possible to construct a block\n+such that an \"empty\" filter will be produced. This is the case of a coinbase\n+transaction that has no data pushes in its output script. In this case, the\n+\"hash\" of said filter is simply \"32 zeroes\". \n+\n+\n+We now introduce two new messages to support the fetching and verification of\n+the filter header chain by light clients. \n+\n+The <code>getcfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| Var-Int\n+| NumBlockLocators\n+| uint64\n+| Number of block locators.\n+|-\n+| NumBlockLocators * 32\n+| BlockLocatorHashes\n+| [NumBlockLocators][32]byte\n+| Block locator hashes, with the same semantics as in <code>getheaders</code>.\n+|-\n+| 32\n+| HashStop\n+| [32]byte\n+| Hash to stop at.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Type of filter header being requested.\n+|}\n+\n+The <code>BlockLocators</code> within the message are to be interpreted\n+identically to the <code>BlockLocators</code> within Bitcoin's\n+<code>getheaders</code> and <code>getblocks</code> messages <ref>https://en.bitcoin.it/wiki/Protocol_documentation</ref>.\n+\n+The <code>cfheaders></code> message MUST be sent in response to a\n+<code>getcfheaders</code> message for a particular block hash. The\n+<code>cfheaders</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Descriptions\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| StopHash\n+| []byte\n+| Block hash for the last filter header returned, for locating the filter headers in the blockchain.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter headers being returned.\n+|-\n+| Var-Int\n+| NumHeaders\n+| uint64\n+| Hash to stop at.\n+|-\n+| NumHeaders * 32\n+| HeaderHashes\n+| [NumHeaders][32]byte\n+| Slice of filter headers.\n+|}\n+\n+=== Compact Filters ===\n+\n+The last set of messages we introduce are for ''fetching'' the compact filters\n+themselves. Light clients can SHOULD use these two messages to request a\n+compact filter for a particular block hash.\n+\n+The <code>getcfilter</code> message is defined as follows:\n+\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the client wishes to fetch a filter.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter requested.\n+|}\n+\n+The <code>cfilter</code> message MUST be sent in response to a\n+<code>getcfilter</code> message for a particular block hash.The\n+<code>cfilter</code> message is defined as follows:\n+{| class=\"wikitable\"\n+! Field Size\n+! Description\n+! Data Type\n+! Comments\n+|-\n+| 32\n+| BlockHash\n+| [32]byte\n+| Block hash of the Bitcoin block for which the filter is being returned.\n+|-\n+| 1\n+| FilterType\n+| byte\n+| Byte identifying the type of filter being returned.\n+|-\n+| Var-Int\n+| NumFilterBytes\n+| uint64\n+| A variable length integer encoding the number of bytes of the filter in the following field.\n+|-\n+| NumFilterBytes\n+| FilterBytes\n+| [NumFilterBytes]byte\n+| The raw compressed compact filter for this block.\n+|}\n+\n+The <code>BlockHash</code> field is included in both messages as this allows\n+easily matching requests against responses, as the responses aren't\n+self-identifying like block headers are (via own hash).\n+\n+The parameters <code>N</code> (the number of elements in the filter) and\n+<code>P</code> (<code>1 << false_positive_rate</code>) are required by the light\n+client in order to properly incrementally decode, query, and validate\n+(reconstruct from Bitcoin block) a compact filter. The parameter <code>N</code>\n+cannot be known ahead of time, therefore we define the serialization of a\n+compact filter of type <code>0x00</code> and <code>0x01</code> as:\n+<pre>\n+N || raw_filter_bytes\n+</pre>\n+where <code>N</code> is serialized as a 32-bit big-endian integer.\n+\n+However, there exists a special case of a <code>null</code> filter. This this\n+case an empty byte slice is transmitted rather than consuming\n+<code>4-bytes</code> to encode the size of zero.\n+\n+However, as the parameter <code>P</code> ''MUST'' be globally agreed upon (for\n+a particular filter type), we define this value ''statically'' for filter\n+types: <code>0x00</code> and <code>0x01</code>. For the two aforementioned\n+filter types, the false positive rate MUST be: <code>20</code>, meaning the\n+parameter <code>P</code> is: <code>2^20</code>, meaning <code>fp=20</code>.\n+This value was chosen as during simulations it was the value that minimized the\n+bandwidth utilized by the expected number of blocks downloaded due to false\n+positives, and the bandwidth used to download the filters themselves. The code\n+along with a demo used for the parameter tuning can be found [here https://github.com/Roasbeef/bips/blob/83b83c78e189be898573e0bfe936dd0c9b99ecb9/gcs_light_client/gentestvectors.go]\n+\n+\n+=== Protocol Version Bump ===\n+\n+As this BIP defines new peer-to-peer behavior, we bump the protocol version by\n+one in order to distinguish the newly defined behavior. Full-nodes implementing\n+this BIP should advertise a protocol version of: <code>70016</code>.\n+\n+== New Wallet Capabilities Enabled ==\n+\n+The new light client mode enables wallet to maintain a very compact client-side\n+index of (possibly) the entire chain. Such an index provides a great degree of\n+utility for wallets, as they're now able to perform tasks such as private key\n+imports and full HD-seed imports without the need of a trusted third-party\n+server. Additionally, the compact client-side chain index also opens up the\n+door to smart contract applications which require agent action in response to\n+on-chain events. Examples of such applications include Lightning.\n+\n+== Backwards Compatability ==\n+\n+This light client protocl is NOT backwards compatible with BIP 37. Full nodes\n+MAY implement both protocols to serve both types of light clients.\n+\n+\n+== Implementation Notes ==\n+\n+This filter header chain SHOULD be utilized by light clients to gain a greater\n+degree of security against bamboozling full-nodes during their initial chain\n+sync. In addition to fetching all the bitcoin headers, light clients\n+implementing this BIP SHOULD also fetch all the ''filter headers'' from\n+''each'' of their connected peers. With these headers, light clients SHOULD\n+efficiently detect nodes that advertise a conflicting filter chain history. To\n+do this, light clients MUST ensure that all nodes return the ``same` filter\n+header hash for a particular block header hash. \n+\n+Light clients MAY use the filter header chain to verify purported filter\n+authenticity when fetching the next set of headers from chain tip. Light\n+clients MAY use the following algorithm to more efficiently verify the\n+authenticity of filters (the naive version would fetch the entire filter from\n+each peer, this version saves bandwidth):\n+\n+<pre>\n+verify_from_tip(tip_block_hash: [32]byte):\n+    let filter_types = {supported_filter_types...}\n+    let connected_peers = {list_of_connected_full_nodes...}\n+\n+    for filter_type in filter_types:\n+\n+        let filter_headers = set()\n+        for peer in connected_peers:\n+            let filter_header = peer.fetch_filter_header(tip_block_hash)\n+            filter_headers.insert(filter_header)\n+\n+        if len(filter_headers) != 1:\n+            // Peers have conflicting filters. The light client should fetch\n+            // each unique filter from the set of peers AND fetch the block. The\n+            // light client can then verify which filter header is correct, and\n+            // BAN the offending peers.\n+\n+        // Otherwise, syncing continues as normal: fetch filter to see if it\n+        // matches any relevant items.\n+</pre>\n+\n+Light clients MAY persistently commit all filter headers to disk, as when\n+lazily fetching filters (due to a historical re-scan or chain analysis),\n+they're able to verify the authenticity of any fetched filters.\n+\n+Full-nodes MAY persistently compute and persist the filter header chain\n+on-disk, just as the regular filters.\n+\n+Zero-length filters are sent without an <code>N</code> value, allowing us to save\n+<code>4-bytes</code>. Clients are able to verify that a filter will be\n+<code>null</code> before requesting it (as it will just be the prior filter\n+header hashed with zero bytes). Clients can take this fact into account in\n+order to save a round trip for <code>null</code> headers.\n+\n+Light clients implementing this proposal SHOULD utilize the\n+<code>sendheaders</code> message. This allow quicker syncing from tip, as rather\n+than sending an <code>inv</code> to announce a new blocks, nodes will instead\n+directly send the <code>headers</code> message. With this, nodes can save a\n+round-trip and immediately request\n+<code>cfheaders</code> from each of their connected peers before ultimately\n+fetching the filter itself.\n+\n+Light client syncing MAY be run in reverse, meaning fetching the regular\n+Bitcoin and filter headers from the ''end'' of the chain and working backwards.\n+This allows a client to nearly ''instantly'' be useable if an application doesn't\n+require immediate access to historical filter data.\n+\n+If fetching blocks directly over the p2p network (rather than via a distinct\n+channel), light clients SHOULD fetch blocks from multiple peers in order to\n+mitigate transaction intersection analysis.\n+\n+Full nodes serving the light clients described in this BIP can be implemented\n+in a completely ''stateless'' manner. Implementations can also make a\n+space/time tradeoff by only holding filters on disk to some particular block\n+horizon. If a filter beyond this horizon is requested, then it can be\n+reconstructed in real time. This allows nodes to save on disk-space, as it's\n+likely the older filters would only be fetched for historical rescans. It is\n+also likely that the past few hundred filters will be fetched mostly frequently\n+by smaller devices (phones, laptops, etc) periodically coming back online after a\n+period of inactivity.\n+\n+It is possible to implementations of this BIP to serve ''other''\n+implementations to a degree. All headers (regular bitcoin, regular, extended)\n+can be served, and also any on-disk filters can also be served to other light\n+clients.\n+\n+Key import and rescan: with lazy <code>filters</code> fetching, having a start\n+block is VERY important to avoid fetching all <code>cfilters</code> starting\n+with block 1 (assume you generate your own filters/headers for the genesis\n+block).\n+\n+== Acknowledgments ==\n+\n+We would like to thank bfd (from the bitcoin-dev mailing list) for bringing the\n+basis of this BIP to our attention, Greg Maxwell for pointing us in the\n+direction of Golomb-Rice coding and fast range optimization, Joseph Poon for\n+suggesting the filter header chain scheme, and Pedro Martelletto for writing the\n+initial indexing code for <code>btcd</code>.\n+\n+We would also like to thank Dave Collins, JJ Jeffrey, Eric Lombrozo for useful\n+discussions.\n+\n+== Reference Implementation ==\n+\n+Light client: [https://github.com/lightninglabs/neutrino]\n+\n+Full-node indexing: https://github.com/Roasbeef/btcd/tree/segwit-cbf\n+\n+Golomb-Rice Coded sets: https://github.com/Roasbeef/btcutil/tree/gcs/gcs\n+\n+== Appendix A ==\n+\n+== Mathematical Background == \n+\n+In the following sections, borrowing from techniques typically used in image\n+and video processing, we describe our chosen encoding for the hash fingerprints\n+of the items in our set of relevant items. In order to compress the items of\n+the set in a lossy manner (creating data-structure capable of probabilistic set\n+membership), we utilize Golomb-Rice codes to encode the ''delta'' between\n+successive hash items within our set. This results in a very compact\n+probabilistic set-membership structure.\n+\n+With a goal of building relevant initiation in the minds of the readers of this\n+document, we first start from the bottom of the abstraction ladder, describing\n+the fundamental components our set encoding relies on.\n+\n+\n+=== Run-Length Encoding ===\n+\n+Run-Length Encoding (or RLE) is typically used in the video/image processing\n+space to losslessly compresses images, or video frames. RLE works by ''omitting''\n+the encoding of ''repeated'' values in a data stream. This achieves lossless\n+compression as repeated items simply aren't transmitted. Instead, a value which\n+represents the ''number of times'' a value repeats is transmitted.\n+\n+Typically RLE takes the form of encoding repeated values in a ''binary'' stream.\n+A simple RLE scheme works as follows:\n+* Encode the run length (number of occurrences) of 0's using <code>k</code> bits.\n+** <code>k</code> acts as fixed length encoding for the length of a run.\n+** This value acts as the maximum encodable run-length.\n+* Transmission of runs of 1's is omitted.\n+* Two 1's in a row are denoted by a zero-length run of zero.\n+\n+As an example, consider the following sequence of bits: \n+<pre>\n+{0}^14 1 {0}^9 11 {0}^20 1 {0}^30 11 {0}^11\n+</pre>\n+\n+The RLE of the bit stream above would be:  \n+\n+<pre>\n+1110 1001 0000 1111 0101 1111 1111 0000 0000 1011\n+</pre>\n+\n+RLE allows one to efficiently encode a data stream in a lossless manner. Due to\n+the encoding of runs, RLE works best when encoding a set with a high degree of\n+redundancy. A careful reader will notice that by using a fix-length encoding\n+for the size of runs, efficiency is lost. Therefore, rather than using a\n+fix-length encoding for the size of a run, we can instead use a ''variable''\n+length encoding for the size of a run. This allows us to compress runs of a\n+large size. To do so, we'll now turn to Golomb-Rice Coding.\n+\n+=== Golomb-Rice Coding ===\n+\n+RLE works well when encoding a data stream that has a high degree of redundancy.\n+However, in our case due to the hashing of items within the compact filter,\n+we'll be dealing with items that are ''uniformly distributed''. We can use this\n+fact to leverage a more efficient encoding scheme based on the distribution of\n+the length of a run. The [https://en.wikipedia.org/wiki/Geometric_distribution\n+Geometric Distribution] represents the probabilities of a number of failures\n+before the first success in a series of Bernoulli trials (yes/no experiments).\n+If our values are i.i.d (independent, identically distributed) distributed of\n+the run-length <code>r</code> can be represented as [6]:\n+<pre>\n+P(r = n) = p^n * (1-p)\n+</pre>\n+Intuitively, this calculates the probability of N zeroes (a run) followed by a\n+single 1 (end of a run). Golomb coding takes advantage of this relationship to\n+efficiently encode integers using a two-tuple. Given a group size of <code>m</code> one\n+can encode an integer as:\n+<pre>\n+n = (q*m) + r\n+  where q is (n / m)\n+   and  r is n % m\n+</pre>\n+\n+[https://en.wikipedia.org/wiki/Golomb_coding Golomb Coding] encodes the two\n+values (<code>q</code> and <code>r</code> for a given integer <code>n</code> as\n+a two-tuple. The first value <code>q</code> is encoded using ''unary'', and the\n+second value <code>r</code> is encoded using a fixed-length series of bits. If\n+<code>m = 2^k</code> for some <code>k</code> then this encoding is a\n+specialized sub-set of Golomb encoding known as Golomb-Rice encoding. In this\n+case, <code>r</code> (the remainder) is the <code>k</code>\n+least-significant-bits of <code>n</code>\n+\n+In this case \"runs\", can be seen as the number of multiples of <code>m</code>\n+that divide into <code>n</code> If an encoded integer is close to the value of\n+<code>m</code> then few bits (in unary) will be used to encode each value.\n+\n+To aid in understanding we provide the following examples of using Golomb-Rice\n+encoding to code integers given <code>m=5</code>\n+<pre>\n+n  = (q, r) = c\n+0  = (0, 0) = 0 00\n+1  = (0, 1) = 0 01\n+2  = (0, 2) = 0 10\n+3  = (0, 3) = 0 110\n+4  = (0, 4) = 0 111\n+5  = (1, 0) = 10 00\n+6  = (1, 1) = 10 01\n+7  = (1, 2) = 10 10\n+8  = (1, 3) = 10 110\n+9  = (1, 4) = 10 111\n+10 = (2, 0) = 110 00\n+</pre>\n+\n+The <code>P</code> value can also be interpreted as the parameter to our\n+Geometric Distribution.  Intuitively, to achieve a false positive rate of 1/32\n+(1/2^5), in a series of queries of items which ''aren't'' in the set, we expect\n+to receive a \"NO\" (false) 32 times, before getting a \"YES\" (true, our false\n+positive). Once again, <code>P</code> MUST be a power of two.\n+\n+\n+== Appendix B ==\n+\n+=== Alternatives ===\n+\n+A number of alternative set encodings were considered before Golomb-Rice coded\n+sets were settled upon. In this appendix section, we'll list a few of the\n+alternatives along with our rationale for not pursuing them.\n+\n+==== Cryptographic Accumulators ====\n+\n+Cryptographic accumulators\n+<ref>https://en.wikipedia.org/wiki/Accumulator_(cryptography)</ref>are a\n+cryptographic data structure that enables (amongst other operations) a one way\n+membership test. One advantage of accumulators are that they are ''constant''\n+size, independent on the number of elements inserted into the accumulator.\n+However, current constructions of cryptographic accumulators require an initial\n+trusted set up. Additionally, accumulators based on the Strong-RSA Assumption\n+require mapping set items to prime representatives in the associated group\n+which can be preemptively expensive.\n+\n+==== Matrix Based Probabilistic Set Datastructures ====",
      "path": "gcs_light_client.mediawiki",
      "position": 1031,
      "original_position": 1031,
      "commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "original_commit_id": "d52f586a1309be04e0297e44fa06f6241780e466",
      "in_reply_to_id": null,
      "user": {
        "login": "jamesob",
        "id": 73197,
        "node_id": "MDQ6VXNlcjczMTk3",
        "avatar_url": "https://avatars.githubusercontent.com/u/73197?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jamesob",
        "html_url": "https://github.com/jamesob",
        "followers_url": "https://api.github.com/users/jamesob/followers",
        "following_url": "https://api.github.com/users/jamesob/following%7B/other_user%7D",
        "gists_url": "https://api.github.com/users/jamesob/gists%7B/gist_id%7D",
        "starred_url": "https://api.github.com/users/jamesob/starred%7B/owner%7D%7B/repo%7D",
        "subscriptions_url": "https://api.github.com/users/jamesob/subscriptions",
        "organizations_url": "https://api.github.com/users/jamesob/orgs",
        "repos_url": "https://api.github.com/users/jamesob/repos",
        "events_url": "https://api.github.com/users/jamesob/events%7B/privacy%7D",
        "received_events_url": "https://api.github.com/users/jamesob/received_events",
        "type": "User",
        "site_admin": false
      },
      "body": "\"Datastructures\" should probably be two words.",
      "created_at": "2017-11-30T23:52:50Z",
      "updated_at": "2017-11-30T23:52:50Z",
      "html_url": "https://github.com/bitcoin/bips/pull/609#discussion_r154235642",
      "author_association": "MEMBER",
      "_links": {
        "self": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/comments/154235642"
        },
        "pull_request": {
          "href": "https://api.github.com/repos/bitcoin/bips/pulls/609"
        }
      },
      "start_line": null,
      "original_start_line": null,
      "start_side": null,
      "line": 1031,
      "original_line": 1031,
      "side": "RIGHT"
    }
  ]
}